{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58799b03-5168-4a6e-a989-140c5de8db7a",
   "metadata": {},
   "source": [
    "# Combine History Files along the Time Dimension\n",
    "\n",
    "We have to perform the sparse reindexing with Zarr files at a later step, since the source NetCDF files are very large and difficult to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14e56d9-de4c-4b5c-b1a2-7e39c09a078c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fsspec\n",
    "\n",
    "import dask.distributed\n",
    "from dask.distributed import Client\n",
    "from ncar_jobqueue import NCARCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb96fee-91df-4685-88e5-29becb8454c7",
   "metadata": {},
   "source": [
    "### Configuration/Tuning Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290b1cd-4b98-4185-8269-3f184321d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and final target folders\n",
    "INPUT_FOLDER = '/glade/scratch/bonnland/DART/ds345.0/lnd'\n",
    "TARGET_FOLDER = '/glade/scratch/bonnland/DART/ds345.0/lnd_zarr/'\n",
    "\n",
    "# Target folder for performance tuning\n",
    "#TARGET_FOLDER = '/glade/scratch/bonnland/DART/ds345.0/ZARR-SCRATCH/'\n",
    "\n",
    "#### For Land h1 variables.\n",
    "# \n",
    "#VARS = ['TSA','GPP', 'GRAINC_TO_FOOD', 'GSSHALN', 'GSSUNLN', \n",
    "#            'NPP', 'NPP_NUPTAKE', 'PLANT_NDEMAND', 'QVEGT', 'TLAI']\n",
    "#TARGET_CHUNKS = {'time': 100, 'pft': 100000}\n",
    "\n",
    "#### For Land h0 variables \n",
    "#   Chunks are small, so increase time chunk to prevent fragmentation.\n",
    "VARS = ['TSA', 'ER', 'EFLX_LH_TOT', 'HR']\n",
    "TARGET_CHUNKS = {'time': 1000, 'lat': 32, 'lon': 32} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf743d-3737-471f-ae32-4c810e95d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to keep metadata during Xarray operations.\n",
    "xr.set_options(keep_attrs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f460d-f1f4-4507-8837-242a01a8e155",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run These Cells for Dask CASPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e214ccf-3b17-4b77-9b0c-5edb631fe60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes is processes PER CORE.\n",
    "# This one works fine.\n",
    "#cluster = NCARCluster(cores=15, processes=1, memory='100GB', project='STDD0003')\n",
    "# This one also works, but occasionally hangs neacr the end.\n",
    "#cluster = NCARCluster(cores=10, processes=1, memory='50GB', project='STDD0003')\n",
    "\n",
    "# For Casper\n",
    "num_cores = 1 #2 #1\n",
    "num_jobs = 50 #25 #4\n",
    "walltime = \"3:00:00\"\n",
    "memory = '10GB'  #'100GB'\n",
    "\n",
    "cluster = NCARCluster(cores=num_cores, processes=1, memory=memory, project='STDD0003', walltime=walltime)\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6504c00-ed72-4b73-a21a-8f3a257f8194",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run These Cells for Dask CHEYENNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71ad93-5102-49c2-b799-ac947db37201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "# Processes is processes PER CORE.\n",
    "# This one works fine.\n",
    "#cluster = NCARCluster(cores=15, processes=1, memory='100GB', project='STDD0003')\n",
    "# This one also works, but occasionally hangs near the end.\n",
    "#cluster = NCARCluster(cores=10, processes=1, memory='50GB', project='STDD0003')\n",
    "\n",
    "# For Cheyenne\n",
    "\n",
    "# Run small set of workers on each node to avoid RAM shortages and Dask crashes.  (tried 8,4)\n",
    "num_nodes = 20 #40\n",
    "num_workers_per_node = 3 #2 #1\n",
    "num_cores_per_node = 4\n",
    "walltime = \"6:00:00\" #\"8:00:00\"\n",
    "\n",
    "cluster = NCARCluster(cores=num_cores_per_node, \n",
    "                      processes=num_workers_per_node, \n",
    "                      memory='109GB', \n",
    "                      walltime=walltime)\n",
    "\n",
    "cluster.scale(jobs=num_nodes)\n",
    "\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a728b3-f612-40de-aa6f-7053e4679851",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642b20d7-d5af-4004-a0d6-c7581ce1f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_index(ds):\n",
    "    \"\"\"Compute the transform from 1D to sparse 6D\n",
    "    \"\"\"\n",
    "    lats = list(ds.pfts1d_lat.astype('float32').data)\n",
    "    lons = list(ds.pfts1d_lon.astype('float32').data)\n",
    "    vegtype = list(ds.pfts1d_itype_veg.data)\n",
    "    coltype = list(ds.pfts1d_itype_col.data)\n",
    "    lunittype = list(ds.pfts1d_itype_lunit.data)\n",
    "    active = list(ds.pfts1d_active.data)\n",
    "    \n",
    "    # Redefine the 'pft' dimension as a multi-index, which will increase the number of dimensions.\n",
    "    index = pd.MultiIndex.from_arrays([lats, lons, vegtype, coltype, lunittype, active], \n",
    "                                  names=('pftlat', 'pftlon', 'vegtype', 'coltype', 'lunittype', 'active'))\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94093a76-028e-4ed7-bf08-12f3074a75ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsify(chunk):\n",
    "    chunk = chunk.unstack(sparse=True)\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87b3f8-2b9f-4c3d-91e7-2b596ce8e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sparse(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert the time value from index to datetime64.  \n",
    "    \"\"\"    \n",
    "    index = compute_index(ds)\n",
    "    ds['pft'] = index\n",
    "\n",
    "    # Drop unneeded variables as soon as possible.\n",
    "    drop_vars = [var for var in ds.data_vars \n",
    "                 if var not in PFT_VARS]\n",
    "    ds = ds.drop_vars(drop_vars)\n",
    "\n",
    "    ds = ds.load()\n",
    "    \n",
    "    ds = ds.chunk(chunks=TARGET_CHUNKS)\n",
    "    \n",
    "    for var in PFT_VARS:\n",
    "        # Try limiting to one timestep.\n",
    "        ds[var] = ds[var].isel(time=0)\n",
    "        ds[var] = xr.map_blocks(sparsify, ds[var])\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e35cbc1-43af-45c9-90f9-f3394311c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert the time value from index to datetime64.  \n",
    "    \"\"\"    \n",
    "    # Drop unneeded variables as soon as possible.\n",
    "    drop_vars = [var for var in ds.data_vars \n",
    "                 if var not in VARS]\n",
    "    ds = ds.drop_vars(drop_vars)\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab0451b-66bb-4af8-887b-4a6af4651a4e",
   "metadata": {},
   "source": [
    "## Create a Zarr Store for each of 80 ensemble members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e64df0-2f6f-4fae-8e48-b25463e8d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(member_id, is_instantaneous):\n",
    "    \"\"\"Returns a list of NetCDF files for an ensemble member.\n",
    "    \"\"\"\n",
    "    padded_id = str(member_id).zfill(4)\n",
    "    if is_instantaneous:\n",
    "        data_filter = f'{INPUT_FOLDER}/{padded_id}/*.clm2_{padded_id}.h0.*.nc'\n",
    "    else:\n",
    "        data_filter = f'{INPUT_FOLDER}/{padded_id}/*.clm2_{padded_id}.h1.*.nc'\n",
    "        #data_filter = f'{INPUT_FOLDER}/{padded_id}/*.clm2_{padded_id}.h1.2015.nc'\n",
    "\n",
    "    file_list = fs.glob(data_filter)\n",
    "    \n",
    "    # For now, remove 2011 files. \n",
    "    file_list = [file for file in file_list if '2011' not in file]\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2e3627-70ad-446e-bd8a-5e2e35794d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(member_id):\n",
    "    \"\"\"Given an integer id for some ensemble member, return a Xarray dataset\n",
    "       created from its history files.\n",
    "    \"\"\"\n",
    "    \n",
    "    #is_instantaneous = False\n",
    "    is_instantaneous = True\n",
    "    file_list = get_file_list(member_id, is_instantaneous)\n",
    "    #print(file_list)\n",
    "\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        ds = xr.open_mfdataset(file_list, concat_dim='time', parallel=True,\n",
    "                               preprocess=preprocess, decode_cf=False, combine=\"nested\",\n",
    "                               data_vars='minimal', coords='minimal', compat='override',\n",
    "                              )\n",
    "\n",
    "    #  engine='h5netcdf'\n",
    "    \n",
    "    # Rechunk after combining time steps, so we can chunk time.\n",
    "    # Note that \"chunks\" specifies the number of elements *in* each chunk,\n",
    "    # not the number of chunks.\n",
    "    ds = ds.chunk(chunks=TARGET_CHUNKS)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d02b2-062e-4052-a9b5-944d6e35a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(ds, member_id):\n",
    "    save_folder = TARGET_FOLDER\n",
    "    store = f'{save_folder}/member_{member_id}.zarr'\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623656e-5e7c-4842-a3f9-3dae5b5cee1f",
   "metadata": {},
   "source": [
    "### Loop over ensemble members and create a Zarr store for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed22692b-cffd-4c6f-a8c7-2999361dc9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fs = fsspec.filesystem(None)\n",
    "\n",
    "#for i in range(1):\n",
    "#for i in np.arange(1, 80):\n",
    "for i in range(80):\n",
    "    member_id = i+1\n",
    "    print(f'  Creating store for member {member_id} ...')\n",
    "    ds = get_dataset(member_id)\n",
    "    save_data(ds, member_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087de4f-87fc-47b0-9a6f-1a098a7ccfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1091727-b6cf-4419-a870-2ff631dfb082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff16d1d-a833-4caa-a54f-f2a1eca0d9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe295e91-6f60-47a0-b865-38bcfbf9af62",
   "metadata": {},
   "source": [
    "### Verify details from one of the created stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3021d8dc-9343-419f-9669-59f5c062c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = '/glade/scratch/bonnland/DART/ds345.0/lnd_zarr/member_1.zarr'\n",
    "ds = xr.open_zarr(store, consolidated=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c36b9f-ed05-40c6-b84b-79b2c14f302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1429ae-0086-4dbb-a428-748a2a8bb78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8981f-8f1b-4a0e-a6f4-1f9ae8516938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde9ff0e-91bc-409a-860b-5a8fb934943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = xr.DataArray(\n",
    "    data=np.ones((2, 3)),\n",
    "    dims=[\"time\", \"pft\"],coords={\"time\": range(2), \"pft\": range(3), \"a\": (\"x\", [3, 4])},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7feae00-7400-4978-bbed-6a5f2770dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df99ff-6e02-4bf0-b700-0c774b9de597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379dfaf-7776-43ef-a1fc-fa16d03cba3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88d3d95d-0b26-40b9-b746-ca393b398adb",
   "metadata": {},
   "source": [
    "## Unused/Non-working Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e73c02-19b3-4598-b9c4-ead3e1af490e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c0ae6-9055-49f3-8eea-87495c09ec5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034dc3e8-3ec7-4bcd-8b3b-8224b7fd4844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0d281a-74ae-4f0e-b76d-2dfb7cb40d44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0ffe4c-d0a4-4e83-96e0-e70c5a8301a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0273d79-39dd-479f-b555-1818e6bb2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This naive approach to reshaping results in out-of-memory errors.\n",
    "\n",
    "def reshape_pft_var(ds, var, pft):\n",
    "    \"\"\"Given a dataset and specific variable name, return a version of the variable's \n",
    "       data with added dimensions for pft, lat, and lon.\n",
    "    \"\"\"\n",
    "    ixy            = ds.pfts1d_ixy\n",
    "    jxy            = ds.pfts1d_jxy\n",
    "    vegtype        = ds.pfts1d_itype_veg    \n",
    "\n",
    "    gridded = np.empty([len(ds.time), len(pft_names), len(ds.lat), len(ds.lon)], dtype=np.float32)\n",
    "    gridded[:, vegtype.values.astype(int)-1, jxy.values.astype(int) - 1, ixy.values.astype(int) - 1] = ds[var].values\n",
    "    \n",
    "    return gridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb555daa-a90a-4975-85bc-589119e186c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7688f89-9500-4c32-a484-d87255fd5de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3178b867-a8ac-44ba-b265-b1fc43ddfd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nonsparse(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert the time value from index to datetime64.  \n",
    "    \"\"\"\n",
    "    # Drop unneeded variables as soon as possible.\n",
    "    drop_vars = [var for var in ds.data_vars \n",
    "                 if var not in PFT_VARS]\n",
    "\n",
    "    ds_fixed = ds.drop_vars(drop_vars)\n",
    "    \n",
    "    # Reshape the dataset to use a grid using Pandas\n",
    "    keep_vars = [var for var in ds.data_vars \n",
    "                 if var in PFT_VARS]\n",
    "\n",
    "    for var in keep_vars:\n",
    "        for pft in pft_names: \n",
    "            new_var = f'{var}__{pft}'\n",
    "            print(new_var)\n",
    "            gridded = reshape_pft_var(ds, var, pft)\n",
    "            ds_fixed[new_var] = xr.DataArray(gridded, dims=['time', 'lat', 'lon'],\n",
    "                                  coords=[ds.time.values, ds.lat.values, ds.lon.values])\n",
    "        \n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45cd190-78ed-4fdd-9dbf-57e028a1ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach keeps the pft-related data sparse, but latitudes without land are not represented in the \n",
    "# new dimension lat_pft.\n",
    "\n",
    "# It may be possible to expand the latitude dimension later to include these missing values:\n",
    "# [-62.67016, -61.72775, -60.78534, -59.842934, -58.900524, -57.958115, -57.015705, \n",
    "#  84.34555, 85.28796, 86.23037, 87.172775, 88.11518, 89.057594, 90.0]\n",
    "\n",
    "def expand_pft_dataset_MYSTERY_FAIL(ds):\n",
    "    lats = ds.pfts1d_lat.astype('float32').data\n",
    "    lons = ds.pfts1d_lon.astype('float32').data\n",
    "    vegtype = ds.pfts1d_itype_veg.data\n",
    "    coltype = ds.pfts1d_itype_col.data\n",
    "    lunittype = ds.pfts1d_itype_lunit.data\n",
    "    active = ds.pfts1d_active.data\n",
    "    \n",
    "    #print(f'{len(lats)}  {len(lons)}  {len(vegtype)}  {len(coltype)}  {len(lunittype)}  {len(active)}')\n",
    "\n",
    "    # Redefine the 'pft' dimension as a multi-index, which will increase the number of dimensions.\n",
    "    #arrays = [list(lats.data), list(lons.data), list(vegtype.data), list(coltype.data), list(lunittype.data, active.data]\n",
    "    #dim_names = ('pftlat', 'pftlon', 'vegtype', 'coltype', 'lunittype', 'active')\n",
    "    index = pd.MultiIndex.from_arrays([lats, lons, vegtype, coltype, lunittype, active], \n",
    "                                  names=('pftlat', 'pftlon', 'vegtype', 'coltype', 'lunittype', 'active'))\n",
    "    ds['pft'] = index\n",
    "\n",
    "    # Keep the data sparse if possible to avoid memory shortages.\n",
    "    ds_new = ds.unstack(sparse=True)\n",
    "\n",
    "    return ds_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
