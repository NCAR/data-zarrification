{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "from distributed.utils import format_bytes\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run These Cells for Dask Processing (NEW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "num_jobs = 15\n",
    "walltime = '1:00:00'\n",
    "memory='60GB' \n",
    "\n",
    "# This line makes the dashboard link work on JupyterHub.\n",
    "dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "\n",
    "cluster = PBSCluster(cores=1, processes=1, walltime=walltime, memory=memory, queue='casper', \n",
    "                     resource_spec='select=1:ncpus=1:mem=10GB',)\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "\n",
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Creating Combined Zarr Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONDecodeError\n",
    "\n",
    "def get_deserialized_json(field):\n",
    "    '''Attempt to deserialize string, and if this fails, return the string.'''\n",
    "    try:\n",
    "        field = json.loads(field)\n",
    "    except JSONDecodeError:\n",
    "        pass\n",
    "    return field\n",
    "\n",
    "\n",
    "def reduce_metadata(metadata, member_ids):\n",
    "    '''Limit metadata to the given member_id fields.'''\n",
    "    keys = set(ds_hist.attrs.keys())\n",
    "    reduced_metadata = {}\n",
    "    for key in keys:\n",
    "        field = get_deserialized_json(metadata[key])\n",
    "\n",
    "        if isinstance(field, dict):\n",
    "            reduced = {id: field[id] for id in member_ids if id in field.keys()}\n",
    "        else:\n",
    "            reduced = field\n",
    "\n",
    "        # Filter out any empty dictionaries.\n",
    "        if reduced:\n",
    "            reduced_metadata[key] = reduced\n",
    "            \n",
    "    return reduced_metadata\n",
    "\n",
    "\n",
    "def combine_metadata(ds_hist, ds_fut, scenario):\n",
    "    '''Take two Xarray datasets, combine their metadata, and add Zarr-specific metadata.'''\n",
    "\n",
    "    # Drop metadata member ids from ds_hist metadata that are not present in ds_fut\n",
    "    member_ids = ds_fut.coords['member_id'].values\n",
    "    hist_attrs = reduce_metadata(ds_hist.attrs, member_ids)\n",
    "    \n",
    "    keys = set(hist_attrs.keys())\n",
    "    keys = keys.union(set(ds_fut.attrs.keys()))\n",
    "\n",
    "    metadata = {}\n",
    "    for key in keys:\n",
    "        if (key in hist_attrs) and (key in ds_fut.attrs):\n",
    "            hist_value = hist_attrs[key]\n",
    "            fut_value = get_deserialized_json(ds_fut.attrs[key])\n",
    "            \n",
    "            # If both stores have identical metadata, assign the metadata unchanged.\n",
    "            if hist_value == fut_value:\n",
    "                metadata[key] = hist_value\n",
    "            else:\n",
    "                # Otherwise, place both versions in a new dictionary.\n",
    "                metadata[key] = {'hist': hist_value, scenario: fut_value}\n",
    "\n",
    "        elif key in ds_hist.attrs:\n",
    "            metadata[key] = {'hist': hist_value}\n",
    "\n",
    "        else:\n",
    "            metadata[key] = {scenario: fut_value}\n",
    "        \n",
    "        # serialize any metadata dictionary to string.\n",
    "        if isinstance(metadata[key], dict):\n",
    "            metadata[key] = json.dumps(metadata[key])\n",
    "\n",
    "    #metadata['zarr-dataset-reference'] = 'For dataset documentation, see DOI https://doi.org/10.5065/D6SJ1JCH'\n",
    "    metadata['zarr-note-time'] = f'Historical data runs 1950 to 2005, future data ({scenario}) runs 2006 to 2100.'\n",
    "    #metadata['zarr-version'] = '1.0'\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Dataset Diagnostic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zarr Save Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(ds, chunks, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")\n",
    "\n",
    "        \n",
    "def zarr_check(directory):\n",
    "    '''Make sure the zarr stores were properly written'''\n",
    "\n",
    "    p = Path(directory)\n",
    "    stores = list(p.rglob(\"*.zarr\"))\n",
    "    #stores = list(p.rglob(\"tasmax.hist-rcp85.day.NAM-22i*.zarr\"))\n",
    "    for store in stores:\n",
    "        try:\n",
    "            ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "            print('\\n')\n",
    "            print(store)\n",
    "            print(ds)\n",
    "            #pprint.pprint(ds.attrs, width=150, compact=True)        \n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Process Zarr Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/glade/scratch/bonnland/na-cordex/zarr/'\n",
    "output_directory = '/glade/scratch/bonnland/na-cordex/zarr-concat/'\n",
    "temp_directory = '/glade/scratch/bonnland/na-cordex/zarr-temp/'\n",
    "\n",
    "# Create concatenations for hist+rcp runs.\n",
    "scenario = 'rcp45'\n",
    "#scenario = 'rcp85'\n",
    "\n",
    "p = Path(input_directory)\n",
    "input_stores = list(p.glob(f'*.{scenario}.*.zarr'))\n",
    "\n",
    "WRITE_OUTPUT = True\n",
    "\n",
    "for store in input_stores:\n",
    "    future_store = store.as_posix()\n",
    "    historical_store = future_store.replace(scenario, 'hist')\n",
    "\n",
    "    # Determine the output store name and location.\n",
    "    output_store_name = future_store.replace(scenario, 'hist-' + scenario)\n",
    "    output_store_name = output_store_name.split('/')[-1]\n",
    "    output_store = output_directory + output_store_name\n",
    "\n",
    "    print(f\"\\n\\nCreating store {output_store_name}\")\n",
    "    if WRITE_OUTPUT:\n",
    "        # Produce output store if it does not exist yet\n",
    "        if not os.path.exists(output_store):\n",
    "            os.makedirs(output_store)\n",
    "        else:\n",
    "            # Store exists; skip to the next case.\n",
    "            continue\n",
    "\n",
    "    ds_hist = xr.open_zarr(historical_store, consolidated=True)\n",
    "    ds_fut = xr.open_zarr(future_store, consolidated=True)\n",
    "    \n",
    "    hist_vars = list(ds_hist.data_vars.keys())\n",
    "    fut_vars = list(ds_fut.data_vars.keys())\n",
    " \n",
    "    # Verify the data variables are the same for both datasets, and there is only one variable.\n",
    "    #print(f'hist_vars = {hist_vars}')\n",
    "    #print(f'fut_vars = {fut_vars}')\n",
    "    assert(hist_vars == fut_vars)\n",
    "    assert(len(hist_vars) == 1)\n",
    "    data_var = hist_vars[0]\n",
    "    \n",
    "    # Determine final chunk sizes.\n",
    "    chunks = dict(zip(ds_fut[data_var].dims, ds_fut[data_var].data.chunksize))\n",
    "    print(chunks)\n",
    "\n",
    "    #hist_members = ds_hist.coords['member_id'].values\n",
    "    #print(f'Historical member ids: {hist_members}')\n",
    "    \n",
    "    # Drop member_ids from ds_hist that are not in ds_fut.\n",
    "    member_ids = ds_fut.coords['member_id'].values\n",
    "    #print(f'Future member ids: {member_ids}')\n",
    "\n",
    "    if len(member_ids) != len(ds_hist.coords['member_id'].values):\n",
    "        ds_hist = ds_hist.sel(member_id = member_ids)\n",
    "        ds_hist = ds_hist.chunk(chunks)\n",
    "        temp_store_name = historical_store.split('/')[-1]\n",
    "        temp_store = temp_directory + temp_store_name\n",
    "        if WRITE_OUTPUT and not os.path.exists(temp_store):\n",
    "            os.makedirs(temp_store)\n",
    "            print(f'\\n\\n  Writing temporary store: {temp_store}...')\n",
    "            save_data(ds_hist, chunks, temp_store)\n",
    "        ds_hist = xr.open_zarr(temp_store, consolidated=True)\n",
    "            \n",
    "            \n",
    "\n",
    "    #hist_members = ds_hist.coords['member_id'].values\n",
    "    #print(f'Modified Historical member ids: {hist_members}')\n",
    "\n",
    "    # Print some diagnostic info to get that warm, fuzzy feeling.\n",
    "    #print_ds_info(ds_hist, hist_vars[0])\n",
    "    #print_ds_info(ds_fut, fut_vars[0])\n",
    "\n",
    "    # Verify that the data variable chunk sizes match for both datasets\n",
    "    #print(f'hist chunksize = {ds_hist[data_var].data.chunksize}')\n",
    "    #print(f'fut chunksize = {ds_fut[data_var].data.chunksize}')\n",
    "    #assert(ds_hist[data_var].data.chunksize == ds_fut[data_var].data.chunksize)\n",
    "    \n",
    "    #print(ds_hist[data_var].data.chunks)\n",
    "    \n",
    "    metadata = combine_metadata(ds_hist, ds_fut, scenario)\n",
    "    #print(f'\\n\\nMetadata for {output_store}:\\n')\n",
    "    #pprint.pprint(metadata, width=150, compact=True)\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        # Combine stores\n",
    "        ds_out = xr.concat([ds_hist, ds_fut], dim='time', coords='minimal').sortby('time')\n",
    "\n",
    "        #if 'height' in ds_out[data_var].coords.keys():\n",
    "        #    ds_out = ds_out.drop_dims('height')\n",
    "        print(ds_out.coords)\n",
    "            \n",
    "        # Delete the existing encoding to avoid later errors.\n",
    "        ds_out[data_var].encoding = {}\n",
    "        #del ds_out.data.encoding['chunks']\n",
    "        #ds_out[data_var].coords.encoding = {}\n",
    "\n",
    "        # De-fragment chunks along the time dimension.\n",
    "        chunks = dict(zip(ds_fut[data_var].dims, ds_fut[data_var].data.chunksize))\n",
    "        print(chunks)\n",
    "        ds_out = ds_out.chunk(chunks)\n",
    "\n",
    "        print(ds_out[data_var].encoding)\n",
    "        \n",
    "        # Print diagnostic info.\n",
    "        print_ds_info(ds_out, hist_vars[0])\n",
    "        \n",
    "        # Assign final metadata\n",
    "        ds_out.attrs = metadata\n",
    "        \n",
    "        # Write the store.\n",
    "        print(f'\\n\\n  Writing store: {output_store}...')\n",
    "        save_data(ds_out, chunks, output_store)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish \"eval\" scenario stores with added metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/glade/scratch/bonnland/na-cordex/zarr/'\n",
    "output_directory = '/glade/scratch/bonnland/na-cordex/zarr-publish/'\n",
    "\n",
    "scenario = 'eval'\n",
    "\n",
    "p = Path(input_directory)\n",
    "input_stores = list(p.glob(f'*.{scenario}.*.zarr'))\n",
    "\n",
    "WRITE_OUTPUT = True\n",
    "\n",
    "for store in input_stores:\n",
    "    store = store.as_posix()\n",
    "\n",
    "    # Determine the output store name and location.\n",
    "    output_store_name = store.split('/')[-1]\n",
    "    output_store = output_directory + output_store_name\n",
    "\n",
    "    print(f\"\\n\\nCreating store {output_store_name}\")\n",
    "    if WRITE_OUTPUT:\n",
    "        # Produce output store if it does not exist yet\n",
    "        if not os.path.exists(output_store):\n",
    "            os.makedirs(output_store)\n",
    "        else:\n",
    "            # Store exists; skip to the next case.\n",
    "            continue\n",
    "\n",
    "    ds = xr.open_zarr(store, consolidated=True)\n",
    "    ds.attrs['zarr-dataset-reference'] = 'For dataset documentation, see DOI https://doi.org/10.5065/D6SJ1JCH'\n",
    "    ds.attrs['zarr-note-time'] = f'ERA-Interim data runs from 1980 to 2014.'\n",
    "    ds.attrs['zarr-version'] = '2.0'\n",
    "    save_data(ds, None, output_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = '/glade/scratch/bonnland/na-cordex/zarr-publish'\n",
    "zarr_check(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Zarr Stores for Learning Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = Path(input_directory)\n",
    "#input_stores = list(p.glob(f'*.{scenario}.*.zarr'))\n",
    "p = Path(output_directory)\n",
    "input_stores = list(p.glob(f'prec.hist-rcp85.day.NAM-22i.*.zarr'))\n",
    "    \n",
    "ds = xr.open_zarr(input_stores[0].as_posix(), consolidated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.data_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['uas'].coords.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds['uas'].coords['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['uas'].coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['uas'].data.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(ds.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-data-zarrification]",
   "language": "python",
   "name": "conda-env-miniconda3-data-zarrification-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
