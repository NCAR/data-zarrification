{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce Zarr Stores for the NA-CORDEX dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "from operator import mul\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cftime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar Conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting single date objects from one type to another.\n",
    "\n",
    "def convert_to_noleap(cftime360_obj, datemap):\n",
    "    ''' Convert Date from 360 Day to NoLeap'''\n",
    "    newdate = datemap[cftime360_obj.dayofyr - 1]\n",
    "    converted = cftime.DatetimeNoLeap(year=cftime360_obj.year, month=newdate.month, day=newdate.day)\n",
    "    return converted\n",
    "\n",
    "def convert_to_gregorian(cftime_noleap_obj):\n",
    "    ''' Convert Date from NoLeap to Gregorian '''\n",
    "    converted = cftime.DatetimeGregorian(year=cftime_noleap_obj.year, month=cftime_noleap_obj.month, day=cftime_noleap_obj.day)\n",
    "    return converted\n",
    "\n",
    "def convert_hour(time_obj, hour_of_day):\n",
    "    ''' Convert date object to Gregorian and explicitly set the hour of day.'''\n",
    "    time_obj = cftime.DatetimeGregorian(year=time_obj.year, month=time_obj.month, day=time_obj.day, hour=hour_of_day, minute=0, second=0)\n",
    "    return time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datemap_360_to_noleap():\n",
    "    ''' Return an array of dates mapping days from the 360-Day calendar to the No-Leap calendar. '''\n",
    "\n",
    "    # Choose any year with 365 days. \n",
    "    dummy_year = 1999\n",
    "\n",
    "    # These are the days of the year that will be missing on the time axis for each year.\n",
    "    # The goal is to spread missing dates out evenly over each year.\n",
    "    #\n",
    "    # Modify specific dates as desired. \n",
    "    missing_dates = [date(dummy_year, 1, 31),\n",
    "                     date(dummy_year, 3, 31),\n",
    "                     date(dummy_year, 5, 31),\n",
    "                     date(dummy_year, 8, 31),\n",
    "                     date(dummy_year, 10, 31),]\n",
    "    \n",
    "    day_one = date(dummy_year, 1, 1)\n",
    "    missing_dates_indexes = [(day - day_one).days + 1 for day in missing_dates] \n",
    "    missing_dates_indexes\n",
    "\n",
    "    datemap_indexes = np.setdiff1d(np.arange(365), missing_dates_indexes)\n",
    "    datemap_indexes\n",
    "\n",
    "    dates = pd.date_range(f'1/1/{dummy_year}', f'12/31/{dummy_year}')\n",
    "    assert(len(dates) == 365)\n",
    "    \n",
    "    date_map = dates[datemap_indexes]\n",
    "    assert(len(date_map) == 360)\n",
    "    \n",
    "    # Check to make sure February 29 is not a date in the resulting map.\n",
    "    #is_leap_day = [(d.month == 2) and (d.day == 29) for d in date_map]\n",
    "    #print(is_leap_day)\n",
    "    #assert(not any(is_leap_day))\n",
    "    return date_map\n",
    "\n",
    "\n",
    "# Create a global map for moving days of the year to other days of the year.\n",
    "datemap_global = get_datemap_360_to_noleap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar Padding Code Used for Cases without Standard Calendars Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_noleap_to_gregorian(ds):\n",
    "    '''Converts an xarray dataset from the NoLeap calendar to the Gregorian calendar.  \n",
    "       Data for Leap Days are filled with missing values (np.nan).\n",
    "    '''\n",
    "    # Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "    ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]\n",
    "    \n",
    "    # Create an equivalent date range on the Gregorian calendar\n",
    "    start_date = ds.time.values[0]\n",
    "    end_date = ds.time.values[-1]\n",
    "    times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian', normalize=True), dims='time')\n",
    "    \n",
    "    # Find the leap days in this date range.\n",
    "    is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "    leap_days = times.where(is_leap_day, drop=True)\n",
    "    \n",
    "    # Create fill values for these days.\n",
    "    one_time_step = ds.isel(time=slice(0, 1))\n",
    "    fill_values = []\n",
    "    for leap_day in leap_days:\n",
    "        d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "        d = d.assign_coords(time=[leap_day.data])\n",
    "        fill_values.append(d)\n",
    "    \n",
    "    ## EXPERIMENTAL SECTION\n",
    "    # Append the fill values to the dataset and then sort values by time.\n",
    "    fill_values.append(ds)\n",
    "    \n",
    "    ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "    #ds_fixed=xr.merge([ds, fill_values]).sortby('time')\n",
    "\n",
    "    # This may be needed for rcp45 case of single ensemble member.\n",
    "    ds_fixed = ds_fixed.assign_coords(time=times)\n",
    "\n",
    "    return ds_fixed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run These Cells for Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# This line makes the dashboard link work on JupyterHub.\n",
    "dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "\n",
    "num_jobs = 15\n",
    "#walltime = \"4:00:00\"\n",
    "walltime = \"0:40:00\"\n",
    "memory = '60GB'\n",
    "\n",
    "cluster = PBSCluster(cores=1, processes=1, walltime=walltime, memory=memory, queue='casper', \n",
    "                     resource_spec='select=1:ncpus=1:mem=10GB',)\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if saving large Zarr files is resulting in KilledWorker or Dask crashes.\n",
    "BIG_SAVE = False\n",
    "if BIG_SAVE:\n",
    "    min_workers = min_jobs\n",
    "    print('Waiting for ' + str(min_jobs) + ' workers.')\n",
    "    client.wait_for_workers(min_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Notebook Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare individual dataset for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert all dataset calendars to Gregorian.  \n",
    "       For now, also drop other data variables, like time bounds, until we get things looking good.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop any time bounds data, as this will get regenerated at a later step.\n",
    "    if 'time_bnds' in ds.data_vars:\n",
    "        ds_fixed = ds.drop('time_bnds')\n",
    "    else:\n",
    "        ds_fixed = ds\n",
    "    \n",
    "    time_values = ds.time.values\n",
    "    \n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "    \n",
    "    # Test for calendar type xarray found when it loaded the dataset.\n",
    "    time_type = f'{type(time_values[0])}'\n",
    "    has_360_day_calendar = \"Datetime360Day\" in time_type\n",
    "    has_noleap_calendar = \"DatetimeNoLeap\" in time_type\n",
    "    \n",
    "    if has_360_day_calendar:\n",
    "        print(f'Found 360 day calendar.\\n')\n",
    "        time_values = [convert_to_noleap(t, datemap_global) for t in time_values]\n",
    "        time_values = [convert_to_gregorian(t) for t in time_values]\n",
    "\n",
    "    # Convert any NoLeap calendar to the Gregorian calendar.\n",
    "    elif has_noleap_calendar:\n",
    "        print(f'Found NoLeap calendar.\\n')\n",
    "        # Include this line when there will be no standard-calendar ensemble members in the final store.\n",
    "        # This call manually inserts missing dates when xarray's fill operation won't work.\n",
    "        ds_fixed = convert_dataset_noleap_to_gregorian(ds_fixed)\n",
    "        time_values = ds_fixed.time.values\n",
    "\n",
    "    # Change time of day to noon for all time axis points.\n",
    "    ###print(ds_fixed.time.values.shape)\n",
    "    time_values = [convert_hour(t, 12) for t in time_values]\n",
    "    ds_fixed = ds_fixed.assign_coords(time = time_values)\n",
    "    \n",
    "    ds_fixed.time.attrs = attrs\n",
    "        \n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged dataset processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(\n",
    "    ds,\n",
    "    start,\n",
    "    end,\n",
    "    freq,\n",
    "    time_bounds_dim,\n",
    "    calendar='standard',\n",
    "    generate_bounds=True,\n",
    "    instantaneous=False,\n",
    "):\n",
    "    '''Regenerate time axis to be consistent with time bounds variable'''\n",
    "    ds = ds.sortby('time').copy()\n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "    \n",
    "    # The bounds name should always be the same.\n",
    "    bounds_name = 'time_bnds'\n",
    "\n",
    "    if generate_bounds:\n",
    "        times = xr.cftime_range(\n",
    "            start=start, end=end, freq=freq, calendar=calendar\n",
    "        )\n",
    "        bounds = np.vstack([times[:-1], times[1:]]).T\n",
    "        ds = ds.assign_coords({bounds_name: ((\"time\", \"bnds\"), bounds)})\n",
    "        \n",
    "    if instantaneous:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].min(time_bounds_dim))\n",
    "    else:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].mean(time_bounds_dim))\n",
    "\n",
    "    ds.time.attrs = attrs\n",
    "    ds.time.encoding = encoding\n",
    "\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_chunking(datasets, chunks, data_var):\n",
    "    \"\"\"Enforce uniform chunking in the Zarr Store.\n",
    "    \"\"\"\n",
    "    dsets = datasets.copy()\n",
    "    choice = random.choice(range(0, len(dsets)))\n",
    "    for i, (key, ds) in enumerate(dsets.items()):\n",
    "        INSERT_LEAP_DAYS = False\n",
    "        if INSERT_LEAP_DAYS:\n",
    "            ds = convert_dataset_noleap_to_gregorian(ds)\n",
    "        print(f'key == {key}')\n",
    "        c = chunks.copy()\n",
    "        for dim in list(c):\n",
    "            if dim not in ds.dims:\n",
    "                del c[dim]\n",
    "        ds = ds.chunk(c)\n",
    "        keys_to_delete = ['intake_esm_dataset_key', 'intake_esm_varname']\n",
    "        for k in keys_to_delete:\n",
    "            del ds.attrs[k]\n",
    "        dsets[key] = ds\n",
    "        #variable = key.split(field_separator)[-1]\n",
    "        print_ds_info(ds, data_var)\n",
    "        #if i == choice:\n",
    "        #    print(ds)\n",
    "        print('\\n')\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n",
    "\n",
    "# For now, make the Zarr output directory a global variable.\n",
    "dirout = '/glade/scratch/bonnland/na-cordex/zarr'\n",
    "\n",
    "def zarr_store(var, scenario, frequency, grid, biascorrection, write=False, dirout=dirout):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f'{dirout}/{var}.{scenario}.{frequency}.{grid}.{biascorrection}.zarr'\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")\n",
    "\n",
    "        \n",
    "def zarr_check():\n",
    "    '''Make sure the zarr stores were properly written'''\n",
    "\n",
    "    from pathlib import Path\n",
    "    p = Path(dirout)\n",
    "    stores = list(p.rglob(\"*.zarr\"))\n",
    "    #stores = list(p.rglob(\"*.rcp45.day.NAM-22i.raw.zarr\"))\n",
    "    for store in stores:\n",
    "        try:\n",
    "            ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "            print('\\n')\n",
    "            print(store)\n",
    "            print(ds)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(dict_in, key, value):\n",
    "    '''Create or append key-value pair to dictionary of lists and return updated dictionary.'''\n",
    "    if key not in dict_in:\n",
    "        dict_in[key] = [value]\n",
    "    else:\n",
    "        dict_in[key].append(value)\n",
    "    return dict_in\n",
    "\n",
    "\n",
    "def is_uniform(metadata, field):\n",
    "    '''Determines if the given field has uniform values across the metadata from different NetCDF files.'''\n",
    "    member_ids = metadata.keys()\n",
    "    values = [metadata[member][field] for member in member_ids if field in metadata[member]]\n",
    "    fileNames = [metadata[member]['fileName'] for member in member_ids if field in metadata[member]]\n",
    "    is_uniform = all(elem == values[0] for elem in values)\n",
    "    return is_uniform, values, fileNames\n",
    "\n",
    "\n",
    "def collect_raw_metadata(data_var, catalog_entries):\n",
    "    '''Grab all available metadata for eventual filtering from a collection of catalog-derived datasets.\n",
    "    '''    \n",
    "    md_global = {}\n",
    "    md_var = {}\n",
    "    md_coords = {}\n",
    "    \n",
    "    # Loop over catalog rows\n",
    "    dataframe = catalog_entries.df\n",
    "    for path, member_id in zip(dataframe['path'], dataframe['member_id']):\n",
    "        ds = xr.open_dataset(path, decode_cf=False)\n",
    "        source_file = path.split('/')[-1]\n",
    "\n",
    "        # Get global metadata\n",
    "        md_global[member_id] = ds.attrs\n",
    "        md_global[member_id]['fileName'] = source_file\n",
    "        \n",
    "        # Copy calendar info to global metadata\n",
    "        md_global[member_id]['original_calendar'] = ds['time'].attrs['calendar']\n",
    "\n",
    "        # Get var metadata\n",
    "        md_var[member_id] = ds[data_var].attrs\n",
    "        md_var[member_id]['fileName'] = source_file\n",
    "\n",
    "        # Get coords metadata\n",
    "        md_coords[member_id] = {coord: ds.coords[coord].attrs for coord in ds.coords}\n",
    "        for coord in ds.coords:\n",
    "            md_coords[member_id][coord]['fileName'] = source_file\n",
    "    \n",
    "    return md_global, md_var, md_coords\n",
    "\n",
    "    \n",
    "def filter_global_metadata(md_global, target_metadata):\n",
    "    ''' Filter global metadata using a whitelist.  Also, record missing and non-uniform values.\n",
    "    '''    \n",
    "    # Record non-uniform entries among the 'keep_first' fields.\n",
    "    non_uniform = {}\n",
    "    for field in target_metadata['keep_first']:\n",
    "        uniform, values, fileNames = is_uniform(md_global, field)\n",
    "        if not uniform:\n",
    "            for value, fileName in zip(values, fileNames):\n",
    "                dict_entry = (value, fileName)\n",
    "                non_uniform = update_dict(non_uniform, field, dict_entry)\n",
    "\n",
    "    # Record missing entries. \n",
    "    missing = {}\n",
    "    for field in target_metadata['keep_first'] + target_metadata['keep_all']:\n",
    "        for member_id in md_global.keys():\n",
    "            if field not in md_global[member_id]:\n",
    "                missing = update_dict(missing, field, md_global[member_id]['fileName'])\n",
    "\n",
    "    # Produce global metadata.\n",
    "    metadata = {}\n",
    "    for field in target_metadata['keep_first']:\n",
    "        for member_id in md_global.keys():\n",
    "            if (field not in metadata) and field in md_global[member_id]: \n",
    "                metadata[field] = md_global[member_id][field]\n",
    "                \n",
    "    for field in target_metadata['keep_all']:\n",
    "        metadata[field] = {}\n",
    "        for member_id in md_global.keys():\n",
    "            if field in md_global[member_id]:\n",
    "                metadata[field][member_id] = md_global[member_id][field]\n",
    "                \n",
    "    # If \"contact_note\" field has no entries, delete the empty dictionary from the metadata.\n",
    "    if \"contact_note\" in metadata and not metadata[\"contact_note\"]:\n",
    "        metadata.pop(\"contact_note\", None)\n",
    "        \n",
    "    # Missing contact_note entries are expected, so remove them.\n",
    "    if \"contact_note\" in missing:\n",
    "        missing.pop(\"contact_note\", None)\n",
    "    \n",
    "    # Serialize non-uniform metadata dictionaries.  \n",
    "    # To eventually turn them back into dictionaries, use the json.loads() function.\n",
    "    for field in target_metadata['keep_all']:\n",
    "        if field in metadata:\n",
    "            metadata[field] = json.dumps(metadata[field])\n",
    "\n",
    "    return metadata, missing, non_uniform\n",
    "\n",
    "\n",
    "def filter_var_coord_metadata(md_var, md_coords, var_coord_metadata):\n",
    "    ''' Filter metadata for variables and coordinates using the whitelist var_coord_metadata. '''\n",
    "\n",
    "    #pprint.pprint(md_var, width=150, compact=True)\n",
    "    #pprint.pprint(md_coords, width=150, compact=True)\n",
    "\n",
    "    # Record diagnostic info about non-uniform values.\n",
    "    non_uniform = {}\n",
    "    for field in var_coord_metadata:\n",
    "        uniform, values, fileNames = is_uniform(md_var, field)\n",
    "        if not uniform:\n",
    "            for value, fileName in zip(values, fileNames):\n",
    "                dict_entry = (value, fileName)\n",
    "                non_uniform = update_dict(non_uniform, field, dict_entry)\n",
    "        \n",
    "    # Initialize consolidated view of variable and coordinate metadata.\n",
    "    var_meta = {}\n",
    "    coord_keys_all = {key for dictkeys in md_coords.keys() for key in md_coords[dictkeys].keys()}\n",
    "    coord_meta = {coord: {} for coord in coord_keys_all}\n",
    "    \n",
    "    # Record diagnostic info about fields that are missing everywhere.\n",
    "    missing = {}\n",
    "    for field in var_coord_metadata:\n",
    "        is_missing = True\n",
    "        for member_id in md_var.keys():\n",
    "            if field in md_var[member_id] and is_missing:\n",
    "                is_missing = False\n",
    "                var_meta[field] = md_var[member_id][field]\n",
    "            for coord in md_coords[member_id].keys():\n",
    "                if field in md_coords[member_id][coord]:\n",
    "                    is_missing = False\n",
    "                    coord_meta[coord][field] = md_coords[member_id][coord][field]\n",
    "        # Record the missing field if it was never found.\n",
    "        if is_missing:\n",
    "            missing = update_dict(missing, field, '(Missing from all source files)')\n",
    "    \n",
    "    # Override the \"calendar\" value for the time coordinate always.\n",
    "    coord_meta['time']['calendar'] = 'gregorian'\n",
    "    \n",
    "    return var_meta, coord_meta, missing, non_uniform\n",
    "\n",
    "\n",
    "\n",
    "def get_all_metadata_from_catalog_entries(data_var, catalog_entries, target_metadata, var_coord_metadata):\n",
    "    '''Take an intake catalog-generated subset and extract target metadata to a single dictionary.\n",
    "    '''\n",
    "\n",
    "    md_global, md_var, md_coords = collect_raw_metadata(data_var, catalog_entries)\n",
    "    g_meta, missing, non_uniform = filter_global_metadata(md_global, target_metadata)\n",
    "\n",
    "    v_meta, c_meta, vc_missing, vc_non_uniform = filter_var_coord_metadata(md_var, md_coords, var_coord_metadata)\n",
    "    \n",
    "    # Combine info from global and non-global metadata diagnostics.\n",
    "    missing.update(vc_missing)\n",
    "    non_uniform.update(vc_non_uniform)\n",
    "    \n",
    "    return g_meta, v_meta, c_meta, missing, non_uniform\n",
    "\n",
    "\n",
    "def write_metadata_output(store_name, global_metadata, var_metadata, coord_metadata, missing, non_uniform):\n",
    "    '''Write the metadata to store_name.out, and warnings to store_name.err'''\n",
    "\n",
    "    # Metadata dictionary fields have to be de-serialized to print nicely.\n",
    "    deserialized_metadata = {}\n",
    "    for key, val in global_metadata.items():\n",
    "        try: \n",
    "            deserialized_metadata[key] = json.loads(val)\n",
    "        except Exception as e:\n",
    "            deserialized_metadata[key] = val\n",
    "            \n",
    "    out = open(f'./zarr-metadata/{store_name}.out', 'w')\n",
    "    pprint.pprint(deserialized_metadata, width=150, stream=out, compact=True)\n",
    "    print(\"var metadata:\", file=out)\n",
    "    pprint.pprint(var_metadata, width=150, stream=out, compact=True)\n",
    "    for coord in coord_metadata.keys():\n",
    "        print(f\"{coord} metadata:\", file=out)\n",
    "        pprint.pprint(coord_metadata[coord], width=150, stream=out, compact=True)\n",
    "    out.close()\n",
    "    \n",
    "    if missing:\n",
    "        err = open(f'./zarr-metadata/{store_name}.missing.err', 'w')\n",
    "        for field in missing.keys():\n",
    "            for file_name in missing[field]:\n",
    "                err.write(f'{field}\\t{file_name}\\n')\n",
    "        err.close\n",
    "\n",
    "    if non_uniform:\n",
    "        err = open(f'./zarr-metadata/{store_name}.inconsistent.err', 'w')\n",
    "        for field in non_uniform.keys():\n",
    "            for (value, file_name) in non_uniform[field]:\n",
    "                err.write(f'{field}\\t\"{value}\"\\t{file_name}\\n')\n",
    "        err.close()\n",
    "            \n",
    "\n",
    "def save_metadata_to_csv(metadata_dict, variable_name):\n",
    "    '''Save metadata in dictionary form to a csv file. '''\n",
    "    dataframe = pd.DataFrame.from_dict(metadata_dict)\n",
    "    dataframe.to_csv(f'{variable_name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Code Using the Configuration File \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a underscore separator, because NA-CORDEX grids have dashes.\n",
    "field_separator = '_'\n",
    "col = intake.open_esm_datastore(\"../../catalogs/glade-na-cordex-bonnland.json\", sep=field_separator)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(col, variable, scenario, frequency, grid, biascorrection, verbose=True):\n",
    "    '''Returns a Zarr Store Creation Spec.'''\n",
    "    query = dict(variable=variable, scenario=scenario, frequency=frequency, grid=grid, biascorrection=biascorrection)\n",
    "    subset = col.search(**query)\n",
    "    if verbose:\n",
    "        print(subset.unique(columns=['variable', 'scenario', 'frequency', 'grid', 'biascorrection']))\n",
    "    return subset, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Configuration File.\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "        \n",
    "variables = config['variables']\n",
    "frequencies = config['frequencies']\n",
    "scenarios = config['scenarios']\n",
    "biascorrections = config['biascorrections']\n",
    "grid_categories = config['grid_categories']\n",
    "\n",
    "target_metadata = config['target_metadata']\n",
    "target_metadata\n",
    "\n",
    "var_coord_metadata = config['var_coord_metadata']\n",
    "var_coord_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a list of Zarr Store Creation Specs.\n",
    "run_config = []\n",
    "for key, value in grid_categories.items():\n",
    "    grid = value['grid']\n",
    "    chunks = value['chunks']\n",
    "    for scenario in scenarios:\n",
    "        for frequency in frequencies:\n",
    "            for biascorrection in biascorrections:\n",
    "                for variable in variables:\n",
    "                    col_subset, query = process_variables(col, variable, scenario, frequency, grid, biascorrection, verbose=False)\n",
    "                    d = {'query': json.dumps(query), 'col': col_subset, 'chunks': chunks, 'frequency': frequency, 'variable': variable}\n",
    "                    if len(col_subset.keys()) > 0:\n",
    "                        run_config.append(d)\n",
    "                    \n",
    "run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_subset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Zarr Stores as specified in config.yaml.\n",
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "    frequency = run['frequency']\n",
    "    chunks = run['chunks'].copy()\n",
    "    var = run['variable']\n",
    "    \n",
    "    qry = json.loads(run['query'])\n",
    "    scen = qry['scenario']\n",
    "    grid = qry['grid']\n",
    "    biascorrection = qry['biascorrection']\n",
    "    \n",
    "    # Skip cases where Zarr stores exist already.\n",
    "    zarr_path = f'{dirout}/{var}.{scen}.{frequency}.{grid}.{biascorrection}.zarr'\n",
    "    if os.path.exists(zarr_path):\n",
    "        print(f'Store exists, skipping: {zarr_path}')\n",
    "        continue\n",
    "\n",
    "    # Gather target Zarr metadata from catalog subset.\n",
    "    global_metadata, var_metadata, coord_metadata, missing, non_uniform = get_all_metadata_from_catalog_entries(var, run['col'], target_metadata, var_coord_metadata)\n",
    "    #pprint.pprint(coord_metadata)\n",
    "    \n",
    "    CHECK_METADATA = False\n",
    "    if CHECK_METADATA:\n",
    "        # Loop over input files and print various metadata fields.\n",
    "        for f in run['col'].df['path']:\n",
    "            print(f)\n",
    "            tf = xr.open_dataset(f)\n",
    "            print(tf.time.attrs)\n",
    "            print(tf.data_vars)\n",
    "            print(tf.coords)\n",
    "            print('############\\n')\n",
    "    \n",
    "    # Try preprocessing, including calendar conversion.\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        dsets = run['col'].to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, progressbar=False)\n",
    "\n",
    "    final_chunks = chunks.copy()\n",
    "    final_chunks['member_id'] = 4\n",
    "\n",
    "    dsets = enforce_chunking(dsets, final_chunks, var)\n",
    "\n",
    "    for key, ds in tqdm(dsets.items(), desc='Creating zarr store if not present'):\n",
    "        print('key: ' + key)\n",
    "        key = key.split(field_separator)\n",
    "        scen, frequency, grid, biascorrection = key[0], key[1], key[2], key[3]\n",
    "\n",
    "\n",
    "        # Regenerate the time bounds variable to be consistent across all ensemble members.\n",
    "        #\n",
    "        # start:  Move the starting bound backward from noon to midnight of the first day.\n",
    "        # end:    Create an extra day for the ending time bound of the last day, and set hour to midnight.\n",
    "        start = convert_hour((ds.time.values[0]), 0)\n",
    "        end = convert_hour(pd.to_datetime(ds.time.values[-1].strftime()) + pd.DateOffset(1), 0)\n",
    "        time_bounds_dim='bnds'\n",
    "        ds_fixed = fix_time(ds, start=start, end=end, freq='D', time_bounds_dim=time_bounds_dim).chunk(final_chunks)\n",
    "        \n",
    "        # Xarray forbids changing some attributes on the time axis, so we leave this coordinate alone.\n",
    "        del coord_metadata['time']\n",
    "        \n",
    "        # Add some Zarr metadata concepts.\n",
    "        global_metadata['zarr-dataset-reference'] = 'For dataset documentation, see DOI https://doi.org/10.5065/D6SJ1JCH'\n",
    "        global_metadata['zarr-version'] = '1.0'  # version 1.0 is for stores made in March 2021.\n",
    "        \n",
    "        # Insert target metadata.\n",
    "        ds_fixed.attrs = global_metadata\n",
    "        ds_fixed[var].attrs = var_metadata\n",
    "        for coord in coord_metadata:\n",
    "            ds_fixed[coord].attrs = coord_metadata[coord]\n",
    "       \n",
    "        store = zarr_store(var, scen, frequency, grid, biascorrection, write=True, dirout=dirout)\n",
    "        save_data(ds_fixed, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if Zarr Stores were saved properly.\n",
    "zarr_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr('/glade/scratch/bonnland/na-cordex/zarr/tas.hist.day.NAM-44i.raw.zarr', consolidated=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/glade/collections/cdg/data/cordex/data/mbcn-gridMET/NAM-22i/day/CanRCM4/CanESM2/rcp45/hurs/hurs.rcp45.CanESM2.CanRCM4.day.NAM-22i.mbcn-gridMET.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If Using Dask on HPC, release the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to print out details about the conda environment.\n",
    "# %load_ext watermark\n",
    "# %watermark -d -iv -m -g -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Single Zarr Store, For Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a underscore separator, because NA-CORDEX grids have dashes.\n",
    "field_separator = '_'\n",
    "col = intake.open_esm_datastore(\"../../catalogs/glade-na-cordex-bonnland.json\", sep=field_separator)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to a single target Zarr store.\n",
    "subset = col.search(variable='tas', scenario='hist', frequency='day', grid='NAM-44i', biascorrection='raw')\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information from input files.\n",
    "for file in subset.df['path']:\n",
    "    ds = xr.open_dataset(file, use_cftime=True)\n",
    "    print(file + '\\n')\n",
    "    print(f'[{ds.time.values[0]},  ..., {ds.time.values[-3]},  {ds.time.values[-2]},  {ds.time.values[-1]}]')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=list(ds.coords)\n",
    "ds.coords[coords[0]].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.coords.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var = 'tas'\n",
    "ds[data_var].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of isolating one entry from the catalog.\n",
    "#ds = col['/Users/bonnland/GitRepos/cesm-lens-zarrification/notebooks/na-cordex/data-subsets/subset_tasmax.rcp85.CanESM2.CRCM5-UQAM.day.NAM-22i.raw.nc_tasmax_rcp85_CanESM2_CRCM5-UQAM_day_NAM-22i_raw_common_CanESM2.CRCM5-UQAM'].to_dask()\n",
    "#dict(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-code the variable name in a global variable for now.\n",
    "variables = ['hurs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate datasets according to the catalog JSON metadata.\n",
    "chunks = {'time': 1000, 'lat': 65, 'lon': 120}\n",
    "dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, progressbar=True)\n",
    "#dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, aggregate=False, progressbar=False)\n",
    "#dset = dsets['rcp85_day_NAM-22i_raw']\n",
    "#dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dsets['hist_day_NAM-22i_raw']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will place all ensemble members in the same chunk.   \n",
    "# Comment out to have each ensemble member in its own chunk.\n",
    "chunks = {'member_id': 4, 'lat': 65, 'lon':120, 'time': 1000}\n",
    "#chunks['member_id'] = 4\n",
    "#chunks['time'] = 20\n",
    "\n",
    "# Take care of ragged edges in original datasets, to optimize chunking strategy.\n",
    "dsets = enforce_chunking(dsets, chunks, variables[0])\n",
    "dsets['hist_day_NAM-22i_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Overwrite the Zarr Stores.\n",
    "dsets['hist_day_NAM-22i_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ds in dsets.items():\n",
    "    print('key: ' + key)\n",
    "    key = key.split(field_separator)\n",
    "    scen, frequency, grid, biascorrection = key[0], key[1], key[2], key[3]\n",
    "    \n",
    "    # Regenerate the time bounds variable to be consistent across all ensemble members.\n",
    "    #\n",
    "    # start:  Move the starting bound backward from noon to midnight of the first day.\n",
    "    # end:    Create an extra day for the ending time bound of the last day, and set hour to midnight.\n",
    "    start = convert_hour((dset.time.values[0]), 0)\n",
    "    end = convert_hour(pd.to_datetime(dset.time.values[-1].strftime()) + pd.DateOffset(1), 0)\n",
    "    time_bounds_dim='bnds'\n",
    "    ds_fixed = fix_time(dset, start=start, end=end, freq='D', time_bounds_dim=time_bounds_dim).chunk(chunks)\n",
    "\n",
    "    var = variables[0]\n",
    "    store = zarr_store(var, scen, frequency, grid, biascorrection, write=True, dirout=dirout)\n",
    "    print(store)\n",
    "    save_data(ds_fixed, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name for name in run['col'].keys()]\n",
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Target Metadata and Warnings for Stores in config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "\n",
    "    var = run['variable']\n",
    "    store_names = [name for name in run['col'].keys()]\n",
    "    \n",
    "    # Filter out bogus stores with no associated NetCDF data.\n",
    "    if len(store_names) == 0:\n",
    "        continue\n",
    "\n",
    "    assert(len(store_names) == 1)\n",
    "    store_name = f'{var}_{store_names[0]}'\n",
    "    store_name = store_name.replace('_', '.')\n",
    "    \n",
    "    # Gather target Zarr metadata from catalog subset.\n",
    "    SKIP_PROBLEMS = False\n",
    "    if SKIP_PROBLEMS:\n",
    "        try:\n",
    "            global_metadata, var_metadata, coord_metadata, missing, non_uniform = get_all_metadata_from_catalog_entries(var, run['col'], target_metadata, var_coord_metadata)\n",
    "            write_metadata_output(store_name, global_metadata, var_metadata, coord_metadata, missing, non_uniform)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Could not produce metadata for store \"{store_name}\", skipping.\\n')\n",
    "    else:\n",
    "        global_metadata, var_metadata, coord_metadata, missing, non_uniform = get_all_metadata_from_catalog_entries(var, run['col'], target_metadata, var_coord_metadata)\n",
    "        write_metadata_output(store_name, global_metadata, var_metadata, coord_metadata, missing, non_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run['col'].df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metadata, var_metadata, coord_metadata, missing, non_uniform = get_all_metadata_from_catalog_entries(var, run['col'], target_metadata, var_coord_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = run['col'].df['path']\n",
    "for p in paths:\n",
    "    print(p.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to Using the Catalog for Preprocessing:  Load Datasets Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_folder = './data-subsets'\n",
    "fileList = os.listdir(subset_folder)\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for f in fileList:\n",
    "    # Create xarray dataset from file.\n",
    "    filePath = f'{subset_folder}/{f}'\n",
    "    ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "    print(filePath)\n",
    "    print(ds)\n",
    "    break\n",
    "    #preprocess(ds)\n",
    "        \n",
    "    datasets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test preprocessing for 360-day calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conditions for 360 calendars\n",
    "filePath = './data-subsets/subset_tasmax.rcp85.HadGEM2-ES.RegCM4.day.NAM-22i.raw.nc'\n",
    "ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed = preprocess(ds)\n",
    "ds_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX: Code Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/glade/collections/cdg/data/cordex/data/raw/NAM-22i/ann/CRCM5-UQAM/MPI-ESM-MR/hist/tasmin/tasmin.hist.MPI-ESM-MR.CRCM5-UQAM.ann.NAM-22i.raw.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(ds.data_vars)[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.lat.values\n",
    "x[8:-1:40].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.lat.isel(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.arange(8,100,10)\n",
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.data_vars)\n",
    "print(ds.attrs['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range on the Gregorian calendar\n",
    "start_date = ds.time.values[0]\n",
    "end_date = ds.time.values[-1]\n",
    "\n",
    "times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian'), dims='time')\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the leap days in this date range.\n",
    "is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "leap_days = times.where(is_leap_day, drop=True)\n",
    "leap_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fill values for these days.\n",
    "one_time_step = ds['tasmax'].isel(time=slice(0, 1))\n",
    "fill_values = []\n",
    "for leap_day in leap_days:\n",
    "    d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "    d = d.assign_coords(time=[leap_day.data])\n",
    "    fill_values.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the fill values to the dataset and then sort values by time.\n",
    "fill_values.append(ds['tasmax'])\n",
    "\n",
    "ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dsets[key].get_index('time') for key in dsets][2][0]\n",
    "[dsets[key].get_index('time') for key in dsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[0].time.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.concat(list(dsets.values())[:3], dim='member_id', combine_attrs='drop', data_vars=['tasmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time.values[0].replace(hour=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.time.indexes[\"time\"].to_datetimeindex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following query to gather all data for one variable.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist','rcp85'], grid='NAM-22i', frequency='day')\n",
    "subset = col.search(variable='tasmax', scenario=['rcp85'], grid='NAM-22i', frequency='day')\n",
    "\n",
    "# Use this to load some 360-day data for conversion to the Gregorian calendar.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist'], grid='NAM-22i', frequency='day', driver='HadGEM2-ES')\n",
    "\n",
    "\n",
    "subset.unique(columns=['rcm', 'driver', 'biascorrection', 'common'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in subset.keys():\n",
    "    print(type(subset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for strange metadata\n",
    "for key in tqdm(subset.keys()):\n",
    "    try:\n",
    "        subset[key](cdf_kwargs={'chunks': {}, 'decode_times': False}).to_dask()\n",
    "    except Exception as e:\n",
    "        print(f'\\tFile:{subset[key].df.path.tolist()} --- Exception: {e}', end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused machine-dependent Dask invocation: Superseded by using NCARCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "machine = 'cheyenne'  # 'casper'\n",
    "if machine == 'cheyenne':\n",
    "    # The following is supposedly set when using NCARCluster\n",
    "    dask.config.set({'distributed.dashboard.link': \"https://jupyterhub.ucar.edu/ch/user/{USER}/proxy/{port}/status\"})\n",
    "    from ncar_jobqueue import NCARCluster\n",
    "    cluster = NCARCluster(cores=10, processes=20, memory='109GB', project='STDD0003')\n",
    "    cluster.scale(jobs=20)\n",
    "else:\n",
    "    # Assume machine is Casper.\n",
    "    dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    cluster = SLURMCluster(cores=8, memory='200GB', project='STDD0003')\n",
    "    cluster.scale(jobs=8)\n",
    "\n",
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_metadata(ds, 'test')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metadata_dict = get_all_metadata_from_catalog_entries(col)\n",
    "save_metadata_to_csv(global_metadata_dict, \"tasmax\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-lens-conversion]",
   "language": "python",
   "name": "conda-env-miniconda3-lens-conversion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0182fc39e85a45219399e957fe691adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13761fa2b8324fd2a9ece35a71f4f6ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_226f6576d26a4a04b7db1f63a81d67b0",
       "max": 7,
       "style": "IPY_MODEL_bc55885d178345d485a2e54827a47926",
       "value": 7
      }
     },
     "20afd54515c14fd5891b3c5fa2c379e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Scale",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_dbc81ae788164df28222a95d09f8e05c"
      }
     },
     "2121b17971ce4a46aed3a0c3b0ea6481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Maximum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9b50ae9a7ab3471d85831948b79cfebc"
      }
     },
     "226f6576d26a4a04b7db1f63a81d67b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26fe76239ce948519e75acbe45e3a7cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a0faccbfb8d4e5b95eb6aebe93d929d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30f5f4c7ea0d4f0aa375e06be9293746": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "150px"
      }
     },
     "34223d7b416145089c93c7d8a58c044a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "35eff6daed234d15aea4688997555389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3b81dd47848d4579b0bc817dc9539cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Adapt",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_3e98b72279604abd9bef4fc8ca32738a"
      }
     },
     "3e98b72279604abd9bef4fc8ca32738a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "410e6595ef11423f81b49b9193faf9c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce928ec4d83c4ff6a0ad9d50fe578c03",
       "style": "IPY_MODEL_34223d7b416145089c93c7d8a58c044a",
       "value": "<h2>SLURMCluster</h2>"
      }
     },
     "57c8221c458645c48099db5f2be789dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Manual Scaling",
        "1": "Adaptive Scaling"
       },
       "children": [
        "IPY_MODEL_9ae48b7119c14b828195d19844943a12",
        "IPY_MODEL_dc327235bd944aeab1aafc20a23ff1a3"
       ],
       "layout": "IPY_MODEL_d156020fe11b4f70b58b42d074f06ca8",
       "selected_index": null
      }
     },
     "5ac713a15f234a6ea183925a72ae9db5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "150px"
      }
     },
     "5e3003728e0542babfcf6305e9cd0483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f47e3d9d819d4701b673db7e2d0e7145",
       "style": "IPY_MODEL_35eff6daed234d15aea4688997555389",
       "value": "<p><b>Dashboard: </b><a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a></p>\n"
      }
     },
     "65d81f01eb2f47cc8bb719561dfad40b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c2f9b26c82294089a73eff87c6acb165",
        "IPY_MODEL_7a8bbcc522c145f0bdb6cd336c3b12d8"
       ],
       "layout": "IPY_MODEL_8117606279b3474e8cf1503333509604"
      }
     },
     "6f9bca40e5174d90a00ed76cbb261c82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9b2a064f16204221aeb43dfad9eedc79",
       "style": "IPY_MODEL_f1d996c5452d4183b9fa04c7c306a6c7",
       "value": " 7/7 [1:56:02&lt;00:00, 994.67s/it]"
      }
     },
     "761a1c358a2b48498a5e6f7744667bac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Workers",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_0182fc39e85a45219399e957fe691adb"
      }
     },
     "7871cd76b7b246afb971da3961f6a154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d2013d29175443b28ac300ea006850e7",
        "IPY_MODEL_57c8221c458645c48099db5f2be789dc"
       ],
       "layout": "IPY_MODEL_ba0c5c3b77464f28ac8ab33f96986134"
      }
     },
     "7a8bbcc522c145f0bdb6cd336c3b12d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2a0faccbfb8d4e5b95eb6aebe93d929d",
       "style": "IPY_MODEL_ed072022e70a46998a953b46fa72cb82",
       "value": " 4/4 [11:33&lt;00:00, 173.32s/it]"
      }
     },
     "7faf3793e39c4094b2d03170c314c0a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13761fa2b8324fd2a9ece35a71f4f6ae",
        "IPY_MODEL_6f9bca40e5174d90a00ed76cbb261c82"
       ],
       "layout": "IPY_MODEL_aaeca4b0644a4cd59e0b03765c2c959e"
      }
     },
     "8117606279b3474e8cf1503333509604": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8bee2cc24f5844329fa26e404f7c2396": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8eb6968752454c2a9e1b0e3de50be20f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a3f9df3bd6b4d29ad030d6fe88c8eae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9ae48b7119c14b828195d19844943a12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_761a1c358a2b48498a5e6f7744667bac",
        "IPY_MODEL_20afd54515c14fd5891b3c5fa2c379e6"
       ],
       "layout": "IPY_MODEL_8bee2cc24f5844329fa26e404f7c2396"
      }
     },
     "9b2a064f16204221aeb43dfad9eedc79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b50ae9a7ab3471d85831948b79cfebc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a1de09204dce4cf78671a06f49037f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aaeca4b0644a4cd59e0b03765c2c959e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba0c5c3b77464f28ac8ab33f96986134": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc55885d178345d485a2e54827a47926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c2f9b26c82294089a73eff87c6acb165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_a1de09204dce4cf78671a06f49037f92",
       "max": 4,
       "style": "IPY_MODEL_f465e02eaf2b401c899fb9bd12265491",
       "value": 4
      }
     },
     "ce928ec4d83c4ff6a0ad9d50fe578c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d156020fe11b4f70b58b42d074f06ca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "500px"
      }
     },
     "d2013d29175443b28ac300ea006850e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ac713a15f234a6ea183925a72ae9db5",
       "style": "IPY_MODEL_26fe76239ce948519e75acbe45e3a7cb",
       "value": "\n<div>\n  <style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n  </style>\n  <table style=\"text-align: right;\">\n    <tr> <th>Workers</th> <td>0</td></tr>\n    <tr> <th>Cores</th> <td>0</td></tr>\n    <tr> <th>Memory</th> <td>0 B</td></tr>\n  </table>\n</div>\n"
      }
     },
     "dbc81ae788164df28222a95d09f8e05c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "dc327235bd944aeab1aafc20a23ff1a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ddf498d4803e4f72949a1eeb286b01e0",
        "IPY_MODEL_2121b17971ce4a46aed3a0c3b0ea6481",
        "IPY_MODEL_3b81dd47848d4579b0bc817dc9539cf2"
       ],
       "layout": "IPY_MODEL_8eb6968752454c2a9e1b0e3de50be20f"
      }
     },
     "ddf498d4803e4f72949a1eeb286b01e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Minimum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9a3f9df3bd6b4d29ad030d6fe88c8eae"
      }
     },
     "e205f5d2a7d6419ca82d3d5cbbbe3892": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_410e6595ef11423f81b49b9193faf9c1",
        "IPY_MODEL_7871cd76b7b246afb971da3961f6a154",
        "IPY_MODEL_5e3003728e0542babfcf6305e9cd0483"
       ],
       "layout": "IPY_MODEL_eb05b25522d245569255c51b368528af"
      }
     },
     "eb05b25522d245569255c51b368528af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed072022e70a46998a953b46fa72cb82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f1d996c5452d4183b9fa04c7c306a6c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f465e02eaf2b401c899fb9bd12265491": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "f47e3d9d819d4701b673db7e2d0e7145": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
