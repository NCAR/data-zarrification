{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/NCAR/cesm-lens-aws/issues/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "from operator import mul\n",
    "import random\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cftime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar Conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting single date objects from one type to another.\n",
    "\n",
    "def convert_to_noleap(cftime360_obj, datemap):\n",
    "    ''' Convert Date from 360 Day to NoLeap'''\n",
    "    newdate = datemap[cftime360_obj.dayofyr - 1]\n",
    "    converted = cftime.DatetimeNoLeap(year=cftime360_obj.year, month=newdate.month, day=newdate.day)\n",
    "    return converted\n",
    "\n",
    "def convert_to_gregorian(cftime_noleap_obj):\n",
    "    ''' Convert Date from NoLeap to Gregorian '''\n",
    "    converted = cftime.DatetimeGregorian(year=cftime_noleap_obj.year, month=cftime_noleap_obj.month, day=cftime_noleap_obj.day)\n",
    "    return converted\n",
    "\n",
    "def convert_hour(time_obj, hour_of_day):\n",
    "    ''' Convert date object to Gregorian and explicitly set the hour of day.'''\n",
    "    time_obj = cftime.DatetimeGregorian(year=time_obj.year, month=time_obj.month, day=time_obj.day, hour=hour_of_day, minute=0, second=0)\n",
    "    return time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datemap_360_to_noleap():\n",
    "    ''' Return an array of dates mapping days from the 360-Day calendar to the No-Leap calendar. '''\n",
    "\n",
    "    # Choose any year with 365 days. \n",
    "    dummy_year = 1999\n",
    "\n",
    "    # These are the days of the year that will be missing on the time axis for each year.\n",
    "    # The goal is to spread missing dates out evenly over each year.\n",
    "    #\n",
    "    # Modify specific dates as desired. \n",
    "    missing_dates = [date(dummy_year, 1, 31),\n",
    "                     date(dummy_year, 3, 31),\n",
    "                     date(dummy_year, 5, 31),\n",
    "                     date(dummy_year, 8, 31),\n",
    "                     date(dummy_year, 10, 31),]\n",
    "    \n",
    "    day_one = date(dummy_year, 1, 1)\n",
    "    missing_dates_indexes = [(day - day_one).days + 1 for day in missing_dates] \n",
    "    missing_dates_indexes\n",
    "\n",
    "    datemap_indexes = np.setdiff1d(np.arange(365), missing_dates_indexes)\n",
    "    datemap_indexes\n",
    "\n",
    "    dates = pd.date_range(f'1/1/{dummy_year}', f'12/31/{dummy_year}')\n",
    "    assert(len(dates) == 365)\n",
    "    \n",
    "    date_map = dates[datemap_indexes]\n",
    "    assert(len(date_map) == 360)\n",
    "    \n",
    "    # Check to make sure February 29 is not a date in the resulting map.\n",
    "    #is_leap_day = [(d.month == 2) and (d.day == 29) for d in date_map]\n",
    "    #print(is_leap_day)\n",
    "    #assert(not any(is_leap_day))\n",
    "    return date_map\n",
    "\n",
    "\n",
    "# Create a global map for moving days of the year to other days of the year.\n",
    "datemap_global = get_datemap_360_to_noleap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calendar Padding Code Used for Cases without Standard Calendars Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset_noleap_to_gregorian(ds):\n",
    "    '''Converts an xarray dataset from the NoLeap calendar to the Gregorian calendar.  \n",
    "       Data for Leap Days are filled with missing values (np.nan).\n",
    "    '''\n",
    "    # Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "    ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]\n",
    "    \n",
    "    # Create an equivalent date range on the Gregorian calendar\n",
    "    start_date = ds.time.values[0]\n",
    "    end_date = ds.time.values[-1]\n",
    "    times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian', normalize=True), dims='time')\n",
    "    \n",
    "    # Find the leap days in this date range.\n",
    "    is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "    leap_days = times.where(is_leap_day, drop=True)\n",
    "    \n",
    "    # Create fill values for these days.\n",
    "    one_time_step = ds.isel(time=slice(0, 1))\n",
    "    fill_values = []\n",
    "    for leap_day in leap_days:\n",
    "        d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "        d = d.assign_coords(time=[leap_day.data])\n",
    "        fill_values.append(d)\n",
    "    \n",
    "    ## EXPERIMENTAL SECTION\n",
    "    # Append the fill values to the dataset and then sort values by time.\n",
    "    fill_values.append(ds)\n",
    "    \n",
    "    ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "    #ds_fixed=xr.merge([ds, fill_values]).sortby('time')\n",
    "\n",
    "    #ds_fixed = ds_fixed.assign_coords(time=times)\n",
    "\n",
    "    return ds_fixed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run These Cells for Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "# Processes is processes PER CORE.\n",
    "# This one works fine.\n",
    "#cluster = NCARCluster(cores=15, processes=1, memory='100GB', project='STDD0003')\n",
    "# This one also works, but occasionally hangs near the end.\n",
    "#cluster = NCARCluster(cores=10, processes=1, memory='50GB', project='STDD0003')\n",
    "\n",
    "num_jobs = 5\n",
    "walltime = \"4:00:00\"\n",
    "cluster = NCARCluster(cores=num_jobs, processes=1, memory='100GB', project='STDD0003', walltime=walltime)\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.adapt(minimum_jobs=4, maximum_jobs=8)\n",
    "#cluster.scale(jobs=14)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if saving large Zarr files is resulting in KilledWorker or Dask crashes.\n",
    "BIG_SAVE = False\n",
    "if BIG_SAVE:\n",
    "    min_workers = min_jobs\n",
    "    print('Waiting for ' + str(min_jobs) + ' workers.')\n",
    "    client.wait_for_workers(min_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Notebook Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare individual dataset for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert all dataset calendars to Gregorian.  \n",
    "       For now, also drop other data variables, like time bounds, until we get things looking good.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop any time bounds data, as this will get regenerated at a later step.\n",
    "    if 'time_bnds' in ds.data_vars:\n",
    "        ds_fixed = ds.drop('time_bnds')\n",
    "    else:\n",
    "        ds_fixed = ds\n",
    "    \n",
    "    time_values = ds.time.values\n",
    "    \n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "\n",
    "    #\"\"\"Drop all unneeded variables and coordinates\"\"\"\n",
    "    #vars_to_drop = [vname for vname in ds.data_vars if vname not in variables]\n",
    "    #coord_vars = [vname for vname in ds.data_vars if 'time' not in ds[vname].dims or 'bnd' in vname]\n",
    "    #ds_fixed = ds.set_coords(coord_vars)\n",
    "    #data_vars_dims = []\n",
    "    #for data_var in ds_fixed.data_vars:\n",
    "    #    data_vars_dims.extend(list(ds_fixed[data_var].dims))\n",
    "    #coords_to_drop = [coord for coord in ds_fixed.coords if coord not in data_vars_dims]\n",
    "    #grid_vars = list(set(vars_to_drop + coords_to_drop) - set(['time', 'time_bound']))\n",
    "    #ds_fixed = ds_fixed.drop(grid_vars)\n",
    "    #if 'history' in ds_fixed.attrs:\n",
    "    #    del ds_fixed.attrs['history']\n",
    "    \n",
    "    # Test for calendar type xarray found when it loaded the dataset.\n",
    "    time_type = f'{type(time_values[0])}'\n",
    "    has_360_day_calendar = \"Datetime360Day\" in time_type\n",
    "    has_noleap_calendar = \"DatetimeNoLeap\" in time_type\n",
    "    \n",
    "    # Extract the time_bnds variable for conversion\n",
    "    #bnds = ds_fixed[bounds_name].values\n",
    "\n",
    "    if has_360_day_calendar:\n",
    "        print(f'Found 360 day calendar.\\n')\n",
    "        time_values = [convert_to_noleap(t, datemap_global) for t in time_values]\n",
    "        time_values = [convert_to_gregorian(t) for t in time_values]\n",
    "\n",
    "    # Convert any NoLeap calendar to the Gregorian calendar.\n",
    "    elif has_noleap_calendar:\n",
    "        print(f'Found NoLeap calendar.\\n')\n",
    "        time_values = [convert_to_gregorian(t) for t in time_values]\n",
    "        #ds_fixed = convert_dataset_noleap_to_gregorian(ds_fixed)\n",
    "\n",
    "    # Change time of day to noon for all time axis points.\n",
    "    ###print(ds_fixed.time.values.shape)\n",
    "    time_values = [convert_hour(t, 12) for t in time_values]\n",
    "    \n",
    "    ds_fixed = ds_fixed.assign_coords(time = time_values)\n",
    "    ds_fixed.time.attrs = attrs\n",
    "        \n",
    "    #ds = ds.set_coords([bounds_name])\n",
    "\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged dataset processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(\n",
    "    ds,\n",
    "    start,\n",
    "    end,\n",
    "    freq,\n",
    "    time_bounds_dim,\n",
    "    calendar='standard',\n",
    "    generate_bounds=True,\n",
    "    instantaneous=False,\n",
    "):\n",
    "    '''Regenerate time axis to be consistent with time bounds variable'''\n",
    "    ds = ds.sortby('time').copy()\n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "    \n",
    "    # The bounds name should always be the same.\n",
    "    bounds_name = 'time_bnds'\n",
    "\n",
    "    if generate_bounds:\n",
    "        times = xr.cftime_range(\n",
    "            start=start, end=end, freq=freq, calendar=calendar\n",
    "        )\n",
    "        bounds = np.vstack([times[:-1], times[1:]]).T\n",
    "        ds = ds.assign_coords({bounds_name: ((\"time\", \"bnds\"), bounds)})\n",
    "        \n",
    "    if instantaneous:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].min(time_bounds_dim))\n",
    "    else:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].mean(time_bounds_dim))\n",
    "\n",
    "    ds.time.attrs = attrs\n",
    "    ds.time.encoding = encoding\n",
    "\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_chunking(datasets, chunks, data_var):\n",
    "    \"\"\"Enforce uniform chunking in the Zarr Store.\n",
    "    \"\"\"\n",
    "    dsets = datasets.copy()\n",
    "    choice = random.choice(range(0, len(dsets)))\n",
    "    for i, (key, ds) in enumerate(dsets.items()):\n",
    "        INSERT_LEAP_DAYS = False\n",
    "        if INSERT_LEAP_DAYS:\n",
    "            ds = convert_dataset_noleap_to_gregorian(ds)\n",
    "        print(f'key == {key}')\n",
    "        c = chunks.copy()\n",
    "        for dim in list(c):\n",
    "            if dim not in ds.dims:\n",
    "                del c[dim]\n",
    "        ds = ds.chunk(c)\n",
    "        keys_to_delete = ['intake_esm_dataset_key', 'intake_esm_varname']\n",
    "        for k in keys_to_delete:\n",
    "            del ds.attrs[k]\n",
    "        dsets[key] = ds\n",
    "        #variable = key.split(field_separator)[-1]\n",
    "        #print_ds_info(ds, variable)\n",
    "        print_ds_info(ds, data_var)\n",
    "        if i == choice:\n",
    "            print(ds)\n",
    "        print('\\n')\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n",
    "\n",
    "# For now, make the Zarr output directory a global variable.\n",
    "dirout = '/glade/scratch/bonnland/na-cordex/zarr'\n",
    "\n",
    "def zarr_store(var, exp, frequency, grid, biascorrection, write=False, dirout=dirout):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f'{dirout}/{var}.{exp}.{frequency}.{grid}.{biascorrection}.zarr'\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")\n",
    "\n",
    "        \n",
    "def zarr_check():\n",
    "    '''Make sure the zarr stores were properly written'''\n",
    "\n",
    "    from pathlib import Path\n",
    "    p = Path(dirout)\n",
    "    #stores = list(p.rglob(\"*.zarr\"))\n",
    "    stores = list(p.rglob(\"*.rcp45.day.NAM-22i.raw.zarr\"))\n",
    "    for store in stores:\n",
    "        try:\n",
    "            ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "            print('\\n')\n",
    "            print(store)\n",
    "            print(ds)\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_metadata(ds, member_id):\n",
    "    '''Convert all dataset metadata to dictionary form and return for filtering.\n",
    "    '''\n",
    "    m_dict = {}\n",
    "    for key, value in ds.attrs.items():\n",
    "        m_dict[key] = value\n",
    "    return m_dict\n",
    "\n",
    "\n",
    "def update_dict(dict_in, key, value):\n",
    "    '''Create or append key-value pair to dictionary and return updated dict'''\n",
    "    if key not in dict_in:\n",
    "        dict_in[key] = [value]\n",
    "    else:\n",
    "        dict_in[key].append(value)\n",
    "    return dict_in\n",
    "\n",
    "\n",
    "def is_uniform(metadata, field):\n",
    "    '''Determines if the given field has uniform values across the raw metadata'''\n",
    "    member_ids = metadata.keys()\n",
    "    values = [metadata[member][field] for member in member_ids if field in metadata[member]]\n",
    "    fileNames = [metadata[member]['fileName'] for member in member_ids if field in metadata[member]]\n",
    "    is_uniform = all(elem == values[0] for elem in values)\n",
    "    return is_uniform, values, fileNames\n",
    "\n",
    "    \n",
    "def collect_raw_metadata(catalog_entries, target_metadata):\n",
    "    '''Take a catalog subset and extract target metadata into a dictionary of dictionaries.\n",
    "    '''    \n",
    "    metadata_raw = {}\n",
    "\n",
    "    # Loop over catalog rows\n",
    "    dataframe = catalog_entries.df\n",
    "    for path, member_id in zip(dataframe['path'], dataframe['member_id']):\n",
    "        #ds = xr.open_dataset(path, decode_cf=False, engine=\"h5netcdf\")\n",
    "        ds = xr.open_dataset(path, decode_cf=False)\n",
    "        metadata_raw[member_id] = ds.attrs\n",
    "        metadata_raw[member_id]['fileName'] = path.split('/')[-1]\n",
    "    \n",
    "    return metadata_raw\n",
    "\n",
    "    \n",
    "def get_metadata_from_catalog_entries(catalog_entries, target_metadata):\n",
    "    '''Take a catalog subset and extract target metadata to a single dictionary.\n",
    "    '''\n",
    "    metadata_raw = collect_raw_metadata(catalog_entries, target_metadata)\n",
    "    metadata = {}\n",
    "    \n",
    "    # Record non-uniform entries among the 'keep_first' fields.\n",
    "    non_uniform = {}\n",
    "    for field in target_metadata['keep_first']:\n",
    "        uniform, values, fileNames = is_uniform(metadata_raw, field)\n",
    "        if not uniform:\n",
    "            for value, fileName in zip(values, fileNames):\n",
    "                dict_entry = (value, fileName)\n",
    "                non_uniform = update_dict(non_uniform, field, dict_entry)\n",
    "\n",
    "    # Record missing entries. \n",
    "    missing = {}\n",
    "    for field in target_metadata['keep_first'] + target_metadata['keep_all']:\n",
    "        for member_id in metadata_raw.keys():\n",
    "            if field not in metadata_raw[member_id]:\n",
    "                missing = update_dict(missing, field, metadata_raw[member_id]['fileName'])\n",
    "\n",
    "    # Produce zarr metadata.\n",
    "    for field in target_metadata['keep_first']:\n",
    "        for member_id in metadata_raw.keys():\n",
    "            if (field not in metadata) and field in metadata_raw[member_id]: \n",
    "                metadata[field] = metadata_raw[member_id][field]\n",
    "                \n",
    "    for field in target_metadata['keep_all']:\n",
    "        metadata[field] = {}\n",
    "        for member_id in metadata_raw.keys():\n",
    "            if field in metadata_raw[member_id]:\n",
    "                metadata[field][member_id] = metadata_raw[member_id][field]\n",
    "                \n",
    "    # If \"contact_note\" field has no entries, delete the empty dictionary from the metadata.\n",
    "    if \"contact_note\" in metadata and not metadata[\"contact_note\"]:\n",
    "        metadata.pop(\"contact_note\", None)\n",
    "        \n",
    "    # Missing contact_note entries are expected, so remove them.\n",
    "    if \"contact_note\" in missing:\n",
    "        missing.pop(\"contact_note\", None)\n",
    "    \n",
    "    \n",
    "    # Serialize non-uniform metadata dictionaries.  \n",
    "    # To eventually turn them back into dictionaries, use the json.loads() function.\n",
    "    for field in target_metadata['keep_all']:\n",
    "        if field in metadata:\n",
    "            metadata[field] = json.dumps(metadata[field])\n",
    "\n",
    "    return metadata, missing, non_uniform\n",
    "\n",
    "\n",
    "def write_metadata_output(store_name, metadata, missing, non_uniform):\n",
    "    '''Write the metadata to store_name.out, and warnings to store_name.err'''\n",
    "\n",
    "    # Metadata dictionary fields have to be de-serialized to print nicely.\n",
    "    deserialized_metadata = {}\n",
    "    for key, val in metadata.items():\n",
    "        try: \n",
    "            deserialized_metadata[key] = json.loads(val)\n",
    "        except Exception as e:\n",
    "            deserialized_metadata[key] = val\n",
    "            \n",
    "    out = open(f'./zarr-metadata/{store_name}.out', 'w')\n",
    "    pprint.pprint(deserialized_metadata, width=150, stream=out, compact=True)\n",
    "    out.close()\n",
    "    \n",
    "    if missing:\n",
    "        err = open(f'./zarr-metadata/{store_name}.missing.err', 'w')\n",
    "        for field in missing.keys():\n",
    "            for file_name in missing[field]:\n",
    "                err.write(f'{field}\\t{file_name}\\n')\n",
    "        err.close\n",
    "\n",
    "    if non_uniform:\n",
    "        err = open(f'./zarr-metadata/{store_name}.inconsistent.err', 'w')\n",
    "        for field in non_uniform.keys():\n",
    "            for (value, file_name) in non_uniform[field]:\n",
    "                err.write(f'{field}\\t\"{value}\"\\t{file_name}\\n')\n",
    "        err.close()\n",
    "            \n",
    "\n",
    "def save_metadata_to_csv(metadata_dict, variable_name):\n",
    "    '''Save metadata in dictionary form to a csv file. '''\n",
    "    dataframe = pd.DataFrame.from_dict(metadata_dict)\n",
    "    dataframe.to_csv(f'{variable_name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Code Using the Configuration File \"config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a underscore separator, because NA-CORDEX grids have dashes.\n",
    "field_separator = '_'\n",
    "col = intake.open_esm_datastore(\"./glade-na-cordex-bonnland.json\", sep=field_separator)\n",
    "#col = intake.open_esm_datastore(\"./bogus-na-cordex.json\", sep=field_separator)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(col, variable, scenario, frequency, grid, biascorrection, verbose=True):\n",
    "    '''Returns a Zarr Store Creation Spec.'''\n",
    "    query = dict(variable=variable, scenario=scenario, frequency=frequency, grid=grid, biascorrection=biascorrection)\n",
    "    subset = col.search(**query)\n",
    "    if verbose:\n",
    "        print(subset.unique(columns=['variable', 'scenario', 'frequency', 'grid', 'biascorrection']))\n",
    "    return subset, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Configuration File.\n",
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "        \n",
    "variables = config['variables']\n",
    "frequencies = config['frequencies']\n",
    "scenarios = config['scenarios']\n",
    "biascorrections = config['biascorrections']\n",
    "grid_categories = config['grid_categories']\n",
    "\n",
    "target_metadata = config['target_metadata']\n",
    "target_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a list of Zarr Store Creation Specs.\n",
    "run_config = []\n",
    "for key, value in grid_categories.items():\n",
    "    grid = value['grid']\n",
    "    chunks = value['chunks']\n",
    "    for scenario in scenarios:\n",
    "        for frequency in frequencies:\n",
    "            for biascorrection in biascorrections:\n",
    "                for variable in variables:\n",
    "                    col_subset, query = process_variables(col, variable, scenario, frequency, grid, biascorrection, verbose=False)\n",
    "                    d = {'query': json.dumps(query), 'col': col_subset, 'chunks': chunks, 'frequency': frequency, 'variable': variable}\n",
    "                    run_config.append(d)\n",
    "                    \n",
    "run_config"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce Zarr Stores as specified in config.yaml.\n",
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "    frequency = run['frequency']\n",
    "    chunks = run['chunks'].copy()\n",
    "    var = run['variable']\n",
    "    \n",
    "    qry = json.loads(run['query'])\n",
    "    exp = qry['scenario']\n",
    "    grid = qry['grid']\n",
    "    biascorrection = qry['biascorrection']\n",
    "    \n",
    "    # Skip cases where Zarr stores exist already.\n",
    "    zarr_path = f'{dirout}/{var}.{exp}.{frequency}.{grid}.{biascorrection}.zarr'\n",
    "    if os.path.exists(zarr_path):\n",
    "        print(f'Store exists, skipping: {zarr_path}')\n",
    "        continue\n",
    "\n",
    "    # Gather target Zarr metadata from catalog subset.\n",
    "    store_metadata, missing, non_uniform = get_metadata_from_catalog_entries(run['col'], target_metadata)\n",
    "    #pprint.pprint(store_metadata)\n",
    "    \n",
    "    CHECK_METADATA = False\n",
    "    if CHECK_METADATA:\n",
    "        # Loop over input files and print various metadata fields.\n",
    "        for f in run['col'].df['path']:\n",
    "            print(f)\n",
    "            tf = xr.open_dataset(f)\n",
    "            print(tf.time.attrs)\n",
    "            print(tf.data_vars)\n",
    "            print(tf.coords)\n",
    "            print('############\\n')\n",
    "    \n",
    "    # Try preprocessing, including calendar conversion.\n",
    "    dsets = run['col'].to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, progressbar=False)\n",
    "\n",
    "    final_chunks = chunks.copy()\n",
    "    final_chunks['member_id'] = 4\n",
    "\n",
    "    dsets = enforce_chunking(dsets, final_chunks, var)\n",
    "\n",
    "    for key, ds in tqdm(dsets.items(), desc='Creating zarr store if not present'):\n",
    "        print('key: ' + key)\n",
    "        key = key.split(field_separator)\n",
    "        exp, frequency, grid, biascorrection = key[0], key[1], key[2], key[3]\n",
    "\n",
    "\n",
    "        # Regenerate the time bounds variable to be consistent across all ensemble members.\n",
    "        #\n",
    "        # start:  Move the starting bound backward from noon to midnight of the first day.\n",
    "        # end:    Create an extra day for the ending time bound of the last day, and set hour to midnight.\n",
    "        start = convert_hour((ds.time.values[0]), 0)\n",
    "        end = convert_hour(pd.to_datetime(ds.time.values[-1].strftime()) + pd.DateOffset(1), 0)\n",
    "        time_bounds_dim='bnds'\n",
    "        ds_fixed = fix_time(ds, start=start, end=end, freq='D', time_bounds_dim=time_bounds_dim).chunk(final_chunks)\n",
    "        \n",
    "        # Insert target metadata.\n",
    "        ds_fixed.attrs = store_metadata\n",
    "       \n",
    "        store = zarr_store(var, exp, frequency, grid, biascorrection, write=True, dirout=dirout)\n",
    "        save_data(ds_fixed, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if Zarr Stores were saved properly.\n",
    "zarr_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If Using Dask on HPC, release the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to print out details about the conda environment.\n",
    "# %load_ext watermark\n",
    "# %watermark -d -iv -m -g -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Single Zarr Store, For Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a underscore separator, because NA-CORDEX grids have dashes.\n",
    "field_separator = '_'\n",
    "col = intake.open_esm_datastore(\"./glade-na-cordex-bonnland.json\", sep=field_separator)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to a single target Zarr store.\n",
    "subset = col.search(variable='sfcWind', scenario='hist', frequency='day', grid='NAM-22i', biascorrection='raw')\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information from input files.\n",
    "for file in subset.df['path']:\n",
    "    ds = xr.open_dataset(file, use_cftime=True)\n",
    "    print(file + '\\n')\n",
    "    print(f'[{ds.time.values[0]},  ..., {ds.time.values[-3]},  {ds.time.values[-2]},  {ds.time.values[-1]}]')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of isolating one entry from the catalog.\n",
    "#ds = col['/Users/bonnland/GitRepos/cesm-lens-zarrification/notebooks/na-cordex/data-subsets/subset_tasmax.rcp85.CanESM2.CRCM5-UQAM.day.NAM-22i.raw.nc_tasmax_rcp85_CanESM2_CRCM5-UQAM_day_NAM-22i_raw_common_CanESM2.CRCM5-UQAM'].to_dask()\n",
    "#dict(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-code the variable name in a global variable for now.\n",
    "variables = ['hurs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate datasets according to the catalog JSON metadata.\n",
    "chunks = {'time': 1000, 'lat': 65, 'lon': 120}\n",
    "dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, progressbar=True)\n",
    "#dsets = subset.to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, aggregate=False, progressbar=False)\n",
    "#dset = dsets['rcp85_day_NAM-22i_raw']\n",
    "#dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = dsets['hist_day_NAM-22i_raw']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will place all ensemble members in the same chunk.   \n",
    "# Comment out to have each ensemble member in its own chunk.\n",
    "chunks = {'member_id': 4, 'lat': 65, 'lon':120, 'time': 1000}\n",
    "#chunks['member_id'] = 4\n",
    "#chunks['time'] = 20\n",
    "\n",
    "# Take care of ragged edges in original datasets, to optimize chunking strategy.\n",
    "dsets = enforce_chunking(dsets, chunks, variables[0])\n",
    "dsets['hist_day_NAM-22i_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Overwrite the Zarr Stores.\n",
    "dsets['hist_day_NAM-22i_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ds in dsets.items():\n",
    "    print('key: ' + key)\n",
    "    key = key.split(field_separator)\n",
    "    exp, frequency, grid, biascorrection = key[0], key[1], key[2], key[3]\n",
    "    \n",
    "    # Regenerate the time bounds variable to be consistent across all ensemble members.\n",
    "    #\n",
    "    # start:  Move the starting bound backward from noon to midnight of the first day.\n",
    "    # end:    Create an extra day for the ending time bound of the last day, and set hour to midnight.\n",
    "    start = convert_hour((dset.time.values[0]), 0)\n",
    "    end = convert_hour(pd.to_datetime(dset.time.values[-1].strftime()) + pd.DateOffset(1), 0)\n",
    "    time_bounds_dim='bnds'\n",
    "    ds_fixed = fix_time(dset, start=start, end=end, freq='D', time_bounds_dim=time_bounds_dim).chunk(chunks)\n",
    "\n",
    "    var = variables[0]\n",
    "    store = zarr_store(var, exp, frequency, grid, biascorrection, write=True, dirout=dirout)\n",
    "    print(store)\n",
    "    save_data(ds_fixed, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [name for name in run['col'].keys()]\n",
    "names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Target Metadata and Warnings for Stores in config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "\n",
    "    var = run['variable']\n",
    "    store_names = [name for name in run['col'].keys()]\n",
    "    \n",
    "    # Filter out bogus stores with no associated NetCDF data.\n",
    "    if len(store_names) == 0:\n",
    "        continue\n",
    "\n",
    "    assert(len(store_names) == 1)\n",
    "    store_name = f'{var}_{store_names[0]}'\n",
    "    store_name = store_name.replace('_', '.')\n",
    "    \n",
    "    # Gather target Zarr metadata from catalog subset.\n",
    "    SKIP_PROBLEMS = False\n",
    "    if SKIP_PROBLEMS:\n",
    "        try:\n",
    "            metadata, missing, non_uniform = get_metadata_from_catalog_entries(run['col'], target_metadata)\n",
    "            write_metadata_output(store_name, metadata, missing, non_uniform)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Could not produce metadata for store \"{store_name}\", skipping.\\n')\n",
    "    else:\n",
    "        metadata, missing, non_uniform = get_metadata_from_catalog_entries(run['col'], target_metadata)\n",
    "        write_metadata_output(store_name, metadata, missing, non_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = run['col'].df['path']\n",
    "for p in paths:\n",
    "    print(p.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to Using the Catalog for Preprocessing:  Load Datasets Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_folder = './data-subsets'\n",
    "fileList = os.listdir(subset_folder)\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for f in fileList:\n",
    "    # Create xarray dataset from file.\n",
    "    filePath = f'{subset_folder}/{f}'\n",
    "    ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "    print(filePath)\n",
    "    print(ds)\n",
    "    break\n",
    "    #preprocess(ds)\n",
    "        \n",
    "    datasets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test preprocessing for 360-day calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conditions for 360 calendars\n",
    "filePath = './data-subsets/subset_tasmax.rcp85.HadGEM2-ES.RegCM4.day.NAM-22i.raw.nc'\n",
    "ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed = preprocess(ds)\n",
    "ds_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX: Code Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/Users/bonnland/GitRepos/cesm-lens-zarrification/notebooks/na-cordex/data-subsets/subset_tasmax.rcp85.MPI-ESM-LR.WRF.day.NAM-22i.raw.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.data_vars)\n",
    "print(ds.attrs['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range on the Gregorian calendar\n",
    "start_date = ds.time.values[0]\n",
    "end_date = ds.time.values[-1]\n",
    "\n",
    "times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian'), dims='time')\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the leap days in this date range.\n",
    "is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "leap_days = times.where(is_leap_day, drop=True)\n",
    "leap_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fill values for these days.\n",
    "one_time_step = ds['tasmax'].isel(time=slice(0, 1))\n",
    "fill_values = []\n",
    "for leap_day in leap_days:\n",
    "    d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "    d = d.assign_coords(time=[leap_day.data])\n",
    "    fill_values.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the fill values to the dataset and then sort values by time.\n",
    "fill_values.append(ds['tasmax'])\n",
    "\n",
    "ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dsets[key].get_index('time') for key in dsets][2][0]\n",
    "[dsets[key].get_index('time') for key in dsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[0].time.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.concat(list(dsets.values())[:3], dim='member_id', combine_attrs='drop', data_vars=['tasmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time.values[0].replace(hour=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.time.indexes[\"time\"].to_datetimeindex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following query to gather all data for one variable.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist','rcp85'], grid='NAM-22i', frequency='day')\n",
    "subset = col.search(variable='tasmax', scenario=['rcp85'], grid='NAM-22i', frequency='day')\n",
    "\n",
    "# Use this to load some 360-day data for conversion to the Gregorian calendar.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist'], grid='NAM-22i', frequency='day', driver='HadGEM2-ES')\n",
    "\n",
    "\n",
    "subset.unique(columns=['rcm', 'driver', 'biascorrection', 'common'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in subset.keys():\n",
    "    print(type(subset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for strange metadata\n",
    "for key in tqdm(subset.keys()):\n",
    "    try:\n",
    "        subset[key](cdf_kwargs={'chunks': {}, 'decode_times': False}).to_dask()\n",
    "    except Exception as e:\n",
    "        print(f'\\tFile:{subset[key].df.path.tolist()} --- Exception: {e}', end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unused machine-dependent Dask invocation: Superseded by using NCARCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "machine = 'cheyenne'  # 'casper'\n",
    "if machine == 'cheyenne':\n",
    "    # The following is supposedly set when using NCARCluster\n",
    "    dask.config.set({'distributed.dashboard.link': \"https://jupyterhub.ucar.edu/ch/user/{USER}/proxy/{port}/status\"})\n",
    "    from ncar_jobqueue import NCARCluster\n",
    "    cluster = NCARCluster(cores=10, processes=20, memory='109GB', project='STDD0003')\n",
    "    cluster.scale(jobs=20)\n",
    "else:\n",
    "    # Assume machine is Casper.\n",
    "    dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "    from dask_jobqueue import SLURMCluster\n",
    "    cluster = SLURMCluster(cores=8, memory='200GB', project='STDD0003')\n",
    "    cluster.scale(jobs=8)\n",
    "\n",
    "from distributed import Client\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_metadata(ds, 'test')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metadata_dict = get_metadata_from_catalog_entries(col)\n",
    "save_metadata_to_csv(global_metadata_dict, \"tasmax\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0182fc39e85a45219399e957fe691adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13761fa2b8324fd2a9ece35a71f4f6ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_226f6576d26a4a04b7db1f63a81d67b0",
       "max": 7,
       "style": "IPY_MODEL_bc55885d178345d485a2e54827a47926",
       "value": 7
      }
     },
     "20afd54515c14fd5891b3c5fa2c379e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Scale",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_dbc81ae788164df28222a95d09f8e05c"
      }
     },
     "2121b17971ce4a46aed3a0c3b0ea6481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Maximum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9b50ae9a7ab3471d85831948b79cfebc"
      }
     },
     "226f6576d26a4a04b7db1f63a81d67b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26fe76239ce948519e75acbe45e3a7cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a0faccbfb8d4e5b95eb6aebe93d929d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30f5f4c7ea0d4f0aa375e06be9293746": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "150px"
      }
     },
     "34223d7b416145089c93c7d8a58c044a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "35eff6daed234d15aea4688997555389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3b81dd47848d4579b0bc817dc9539cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Adapt",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_3e98b72279604abd9bef4fc8ca32738a"
      }
     },
     "3e98b72279604abd9bef4fc8ca32738a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "410e6595ef11423f81b49b9193faf9c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce928ec4d83c4ff6a0ad9d50fe578c03",
       "style": "IPY_MODEL_34223d7b416145089c93c7d8a58c044a",
       "value": "<h2>SLURMCluster</h2>"
      }
     },
     "57c8221c458645c48099db5f2be789dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Manual Scaling",
        "1": "Adaptive Scaling"
       },
       "children": [
        "IPY_MODEL_9ae48b7119c14b828195d19844943a12",
        "IPY_MODEL_dc327235bd944aeab1aafc20a23ff1a3"
       ],
       "layout": "IPY_MODEL_d156020fe11b4f70b58b42d074f06ca8",
       "selected_index": null
      }
     },
     "5ac713a15f234a6ea183925a72ae9db5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "150px"
      }
     },
     "5e3003728e0542babfcf6305e9cd0483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f47e3d9d819d4701b673db7e2d0e7145",
       "style": "IPY_MODEL_35eff6daed234d15aea4688997555389",
       "value": "<p><b>Dashboard: </b><a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a></p>\n"
      }
     },
     "65d81f01eb2f47cc8bb719561dfad40b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c2f9b26c82294089a73eff87c6acb165",
        "IPY_MODEL_7a8bbcc522c145f0bdb6cd336c3b12d8"
       ],
       "layout": "IPY_MODEL_8117606279b3474e8cf1503333509604"
      }
     },
     "6f9bca40e5174d90a00ed76cbb261c82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9b2a064f16204221aeb43dfad9eedc79",
       "style": "IPY_MODEL_f1d996c5452d4183b9fa04c7c306a6c7",
       "value": " 7/7 [1:56:02&lt;00:00, 994.67s/it]"
      }
     },
     "761a1c358a2b48498a5e6f7744667bac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Workers",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_0182fc39e85a45219399e957fe691adb"
      }
     },
     "7871cd76b7b246afb971da3961f6a154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d2013d29175443b28ac300ea006850e7",
        "IPY_MODEL_57c8221c458645c48099db5f2be789dc"
       ],
       "layout": "IPY_MODEL_ba0c5c3b77464f28ac8ab33f96986134"
      }
     },
     "7a8bbcc522c145f0bdb6cd336c3b12d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2a0faccbfb8d4e5b95eb6aebe93d929d",
       "style": "IPY_MODEL_ed072022e70a46998a953b46fa72cb82",
       "value": " 4/4 [11:33&lt;00:00, 173.32s/it]"
      }
     },
     "7faf3793e39c4094b2d03170c314c0a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13761fa2b8324fd2a9ece35a71f4f6ae",
        "IPY_MODEL_6f9bca40e5174d90a00ed76cbb261c82"
       ],
       "layout": "IPY_MODEL_aaeca4b0644a4cd59e0b03765c2c959e"
      }
     },
     "8117606279b3474e8cf1503333509604": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8bee2cc24f5844329fa26e404f7c2396": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8eb6968752454c2a9e1b0e3de50be20f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a3f9df3bd6b4d29ad030d6fe88c8eae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9ae48b7119c14b828195d19844943a12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_761a1c358a2b48498a5e6f7744667bac",
        "IPY_MODEL_20afd54515c14fd5891b3c5fa2c379e6"
       ],
       "layout": "IPY_MODEL_8bee2cc24f5844329fa26e404f7c2396"
      }
     },
     "9b2a064f16204221aeb43dfad9eedc79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b50ae9a7ab3471d85831948b79cfebc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a1de09204dce4cf78671a06f49037f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aaeca4b0644a4cd59e0b03765c2c959e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba0c5c3b77464f28ac8ab33f96986134": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc55885d178345d485a2e54827a47926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c2f9b26c82294089a73eff87c6acb165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_a1de09204dce4cf78671a06f49037f92",
       "max": 4,
       "style": "IPY_MODEL_f465e02eaf2b401c899fb9bd12265491",
       "value": 4
      }
     },
     "ce928ec4d83c4ff6a0ad9d50fe578c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d156020fe11b4f70b58b42d074f06ca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "500px"
      }
     },
     "d2013d29175443b28ac300ea006850e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ac713a15f234a6ea183925a72ae9db5",
       "style": "IPY_MODEL_26fe76239ce948519e75acbe45e3a7cb",
       "value": "\n<div>\n  <style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n  </style>\n  <table style=\"text-align: right;\">\n    <tr> <th>Workers</th> <td>0</td></tr>\n    <tr> <th>Cores</th> <td>0</td></tr>\n    <tr> <th>Memory</th> <td>0 B</td></tr>\n  </table>\n</div>\n"
      }
     },
     "dbc81ae788164df28222a95d09f8e05c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "dc327235bd944aeab1aafc20a23ff1a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ddf498d4803e4f72949a1eeb286b01e0",
        "IPY_MODEL_2121b17971ce4a46aed3a0c3b0ea6481",
        "IPY_MODEL_3b81dd47848d4579b0bc817dc9539cf2"
       ],
       "layout": "IPY_MODEL_8eb6968752454c2a9e1b0e3de50be20f"
      }
     },
     "ddf498d4803e4f72949a1eeb286b01e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Minimum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9a3f9df3bd6b4d29ad030d6fe88c8eae"
      }
     },
     "e205f5d2a7d6419ca82d3d5cbbbe3892": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_410e6595ef11423f81b49b9193faf9c1",
        "IPY_MODEL_7871cd76b7b246afb971da3961f6a154",
        "IPY_MODEL_5e3003728e0542babfcf6305e9cd0483"
       ],
       "layout": "IPY_MODEL_eb05b25522d245569255c51b368528af"
      }
     },
     "eb05b25522d245569255c51b368528af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed072022e70a46998a953b46fa72cb82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f1d996c5452d4183b9fa04c7c306a6c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f465e02eaf2b401c899fb9bd12265491": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "f47e3d9d819d4701b673db7e2d0e7145": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
